{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNgyMvKsId+JtORQs5L47em"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z18GtbO0lDj8","executionInfo":{"status":"ok","timestamp":1760287444250,"user_tz":-330,"elapsed":21879,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"5792d8d0-c14b-4272-c69d-18e5c81225d7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import pandas as pd\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base directory\n","base_dir = '/content/drive/MyDrive/amazon_ml_challenge'"]},{"cell_type":"code","source":["import torch\n","# Load your CLIP-processed test data\n","test_data_path = '/content/drive/MyDrive/amazon_ml_challenge/combined_CLIP_final/clip_full_with_alignment.pt'\n","test_data = torch.load(test_data_path)\n","train_input = test_data['train_input']\n","val_input = test_data['val_input']\n","train_targets = test_data['train_targets']\n","val_targets = test_data['val_targets']\n"],"metadata":{"id":"lBjGLoQ1pBnm","executionInfo":{"status":"ok","timestamp":1760287453853,"user_tz":-330,"elapsed":9601,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# =========================================================================\n","# PRICE-BASED SPECIALIST MODELS - PHASE 0 & 1\n","# Phase 0: Data Preparation & Binning\n","# Phase 1: Train Specialist Ensembles\n","# =========================================================================\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import mean_absolute_error\n","from tqdm import tqdm\n","import os\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Using device: {device}\")\n","\n","# =========================================================================\n","# PHASE 0: DATA PREPARATION\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üì¶ PHASE 0: DATA PREPARATION & PRICE BINNING\")\n","print(\"=\"*70)\n","\n","# Assuming you already have:\n","# train_input, val_input, train_targets, val_targets loaded\n","\n","print(f\"\\n‚úÖ Data loaded:\")\n","print(f\"  Train: {train_input.shape}\")\n","print(f\"  Val: {val_input.shape}\")\n","\n","# Convert log prices to original scale\n","train_prices_orig = np.expm1(train_targets.cpu().numpy().squeeze())\n","val_prices_orig = np.expm1(val_targets.cpu().numpy().squeeze())\n","\n","print(f\"\\nüìä Price Statistics:\")\n","print(f\"  Train prices - Min: ${train_prices_orig.min():.2f}, Max: ${train_prices_orig.max():.2f}\")\n","print(f\"  Train prices - Mean: ${train_prices_orig.mean():.2f}, Median: ${np.median(train_prices_orig):.2f}\")\n","\n","# Define price bins\n","BIN_BOUNDARIES = {\n","    'affordable': (0, 50),      # Bin 0\n","    'mid': (50, 100),           # Bin 1\n","    'premium': (100, float('inf'))  # Bin 2\n","}\n","\n","print(f\"\\nüóÇÔ∏è  Price Bin Definitions:\")\n","print(f\"  Bin 0 (Affordable): $0 - $50\")\n","print(f\"  Bin 1 (Mid-range): $50 - $100\")\n","print(f\"  Bin 2 (Premium): $100+\")\n","\n","# Create masks for training data\n","train_affordable_mask = train_prices_orig < 40\n","train_mid_mask = (train_prices_orig >= 40) & (train_prices_orig < 75)\n","train_premium_mask = train_prices_orig >= 75\n","\n","# Create masks for validation data\n","val_affordable_mask = val_prices_orig < 40\n","val_mid_mask = (val_prices_orig >= 40) & (val_prices_orig < 75)\n","val_premium_mask = val_prices_orig >= 75\n","\n","# Print distribution\n","print(f\"\\nüìà Training Data Distribution:\")\n","print(f\"  Affordable (<$50):   {train_affordable_mask.sum():6d} samples ({train_affordable_mask.sum()/len(train_prices_orig)*100:5.1f}%)\")\n","print(f\"  Mid ($50-$100):      {train_mid_mask.sum():6d} samples ({train_mid_mask.sum()/len(train_prices_orig)*100:5.1f}%)\")\n","print(f\"  Premium ($100+):     {train_premium_mask.sum():6d} samples ({train_premium_mask.sum()/len(train_prices_orig)*100:5.1f}%)\")\n","\n","print(f\"\\nüìà Validation Data Distribution:\")\n","print(f\"  Affordable (<$50):   {val_affordable_mask.sum():6d} samples ({val_affordable_mask.sum()/len(val_prices_orig)*100:5.1f}%)\")\n","print(f\"  Mid ($50-$100):      {val_mid_mask.sum():6d} samples ({val_mid_mask.sum()/len(val_prices_orig)*100:5.1f}%)\")\n","print(f\"  Premium ($100+):     {val_premium_mask.sum():6d} samples ({val_premium_mask.sum()/len(val_prices_orig)*100:5.1f}%)\")\n","\n","# Split data by bins\n","print(f\"\\nüî™ Splitting data by price bins...\")\n","\n","# Training splits\n","train_affordable_input = train_input[train_affordable_mask]\n","train_affordable_targets = train_targets[train_affordable_mask]\n","\n","train_mid_input = train_input[train_mid_mask]\n","train_mid_targets = train_targets[train_mid_mask]\n","\n","train_premium_input = train_input[train_premium_mask]\n","train_premium_targets = train_targets[train_premium_mask]\n","\n","# Validation splits\n","val_affordable_input = val_input[val_affordable_mask]\n","val_affordable_targets = val_targets[val_affordable_mask]\n","\n","val_mid_input = val_input[val_mid_mask]\n","val_mid_targets = val_targets[val_mid_mask]\n","\n","val_premium_input = val_input[val_premium_mask]\n","val_premium_targets = val_targets[val_premium_mask]\n","\n","print(f\"‚úÖ Data splitting complete!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"epFeBOlBvOUW","executionInfo":{"status":"ok","timestamp":1760289795152,"user_tz":-330,"elapsed":277,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"b82e17bf-5009-4cdd-e2b8-564e839c4b8f"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","======================================================================\n","üì¶ PHASE 0: DATA PREPARATION & PRICE BINNING\n","======================================================================\n","\n","‚úÖ Data loaded:\n","  Train: torch.Size([60000, 1046])\n","  Val: torch.Size([15000, 1046])\n","\n","üìä Price Statistics:\n","  Train prices - Min: $0.13, Max: $1280.00\n","  Train prices - Mean: $23.60, Median: $14.09\n","\n","üóÇÔ∏è  Price Bin Definitions:\n","  Bin 0 (Affordable): $0 - $50\n","  Bin 1 (Mid-range): $50 - $100\n","  Bin 2 (Premium): $100+\n","\n","üìà Training Data Distribution:\n","  Affordable (<$50):    50903 samples ( 84.8%)\n","  Mid ($50-$100):        6056 samples ( 10.1%)\n","  Premium ($100+):       3041 samples (  5.1%)\n","\n","üìà Validation Data Distribution:\n","  Affordable (<$50):    12743 samples ( 85.0%)\n","  Mid ($50-$100):        1494 samples ( 10.0%)\n","  Premium ($100+):        763 samples (  5.1%)\n","\n","üî™ Splitting data by price bins...\n","‚úÖ Data splitting complete!\n"]}]},{"cell_type":"code","source":["# =========================================================================\n","# MODEL DEFINITION\n","# =========================================================================\n","class RegressionMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","# =========================================================================\n","# SMAPE LOSS FUNCTIONS\n","# =========================================================================\n","def smape_loss(predictions, targets):\n","    \"\"\"SMAPE loss for training - optimizes for SMAPE metric\"\"\"\n","    numerator = torch.abs(predictions - targets)\n","    denominator = (torch.abs(predictions) + torch.abs(targets)) / 2\n","    return torch.mean(numerator / (denominator + 1e-8))\n","\n","def weighted_smape_loss(predictions, targets, premium_threshold=np.log1p(85)):\n","    \"\"\"\n","    Weighted SMAPE loss - 3x penalty for premium items (>$85)\n","    This forces the model to focus more on expensive items\n","    \"\"\"\n","    smape = torch.abs(predictions - targets) / ((torch.abs(predictions) + torch.abs(targets))/2 + 1e-8)\n","\n","    # 3x weight for premium items, 1.5x for mid-range\n","    weights = torch.where(targets > premium_threshold, 5.0,\n","                         torch.where(targets > np.log1p(40), 1.5, 1.0))\n","\n","    return torch.mean(weights * smape)\n","\n","def smape_metric(y_true_log, y_pred_log):\n","    \"\"\"Calculate SMAPE in original scale for validation\"\"\"\n","    y_true = np.expm1(y_true_log)\n","    y_pred = np.expm1(y_pred_log)\n","    return np.mean(np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2 + 1e-8)) * 100\n","\n","# =========================================================================\n","# TRAINING FUNCTION WITH SMAPE OPTIMIZATION\n","# =========================================================================\n","def train_ensemble(X_train, y_train, X_val, y_val,\n","                   n_models, bin_name, epochs=30, lr=1e-3, use_weighted=True,\n","                   model_class=None):\n","    \"\"\"\n","    Train an ensemble of models optimized for SMAPE metric\n","\n","    Args:\n","        use_weighted: If True, use weighted SMAPE (3x weight on premium items)\n","                     If False, use standard SMAPE loss\n","        model_class: Custom model class (if None, uses RegressionMLP)\n","    \"\"\"\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"üöÄ Training {bin_name.upper()} Ensemble ({n_models} models)\")\n","    print(f\"   Loss: {'Weighted SMAPE' if use_weighted else 'SMAPE'}\")\n","    print(f\"{'='*70}\")\n","    print(f\"Training samples: {X_train.shape[0]}\")\n","    print(f\"Validation samples: {X_val.shape[0]}\")\n","\n","    models = []\n","    val_smapes = []\n","    val_maes = []\n","\n","    # Move data to device\n","    X_train = X_train.to(device)\n","    y_train = y_train.to(device)\n","    X_val = X_val.to(device)\n","    y_val = y_val.to(device)\n","\n","    # For validation metrics (original scale)\n","    val_true_orig = np.expm1(y_val.cpu().numpy().squeeze())\n","\n","    for seed in range(n_models):\n","        torch.manual_seed(seed)\n","        np.random.seed(seed)\n","\n","        print(f\"\\nüîÑ Training model {seed+1}/{n_models} (seed={seed})...\")\n","\n","        # Initialize model (use custom class if provided)\n","        ModelClass = model_class if model_class is not None else RegressionMLP\n","        model = ModelClass(X_train.shape[1]).to(device)\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n","\n","        # Choose loss function\n","        if use_weighted:\n","            criterion = weighted_smape_loss\n","        else:\n","            criterion = smape_loss\n","\n","        # Data loader\n","        train_loader = DataLoader(\n","            TensorDataset(X_train, y_train),\n","            batch_size=256,\n","            shuffle=True\n","        )\n","\n","        # Training loop\n","        best_val_smape = float('inf')\n","        patience_counter = 0\n","        patience_limit = 5\n","\n","        for epoch in range(epochs):\n","            model.train()\n","            epoch_loss = 0\n","\n","            for xb, yb in train_loader:\n","                xb, yb = xb.to(device), yb.to(device)\n","                optimizer.zero_grad()\n","                preds = model(xb)\n","                loss = criterion(preds, yb)\n","                loss.backward()\n","                optimizer.step()\n","                epoch_loss += loss.item()\n","\n","            # Validation every 10 epochs\n","            if (epoch + 1) % 10 == 0 or epoch == epochs - 1:\n","                model.eval()\n","                with torch.no_grad():\n","                    val_preds = model(X_val)\n","\n","                    # Clip predictions to reasonable range\n","                    # Min: $3 (log1p(3) ‚âà 1.39), Max: $500 (log1p(500) ‚âà 6.21)\n","                    val_preds_clipped = torch.clamp(val_preds,\n","                                                    min=np.log1p(3),\n","                                                    max=np.log1p(3000))\n","\n","                    # Calculate SMAPE (primary metric)\n","                    val_smape = smape_metric(y_val.cpu().numpy().squeeze(),\n","                                           val_preds_clipped.cpu().numpy().squeeze())\n","\n","                    # Also calculate MAE for reference\n","                    val_preds_orig = np.expm1(val_preds_clipped.cpu().numpy().squeeze())\n","                    val_mae = mean_absolute_error(val_true_orig, val_preds_orig)\n","\n","                avg_loss = epoch_loss / len(train_loader)\n","                print(f\"   Epoch {epoch+1}/{epochs}: Loss = {avg_loss:.4f}, \"\n","                      f\"Val SMAPE = {val_smape:.2f}%, Val MAE = ${val_mae:.2f}\")\n","\n","                # Early stopping based on SMAPE (not MAE!)\n","                if val_smape < best_val_smape:\n","                    best_val_smape = val_smape\n","                    patience_counter = 0\n","                else:\n","                    patience_counter += 1\n","                    if patience_counter >= patience_limit:\n","                        print(f\"   ‚è∏Ô∏è  Early stopping triggered at epoch {epoch+1}\")\n","                        break\n","\n","        models.append(model)\n","        val_smapes.append(best_val_smape)\n","        val_maes.append(val_mae)\n","        print(f\"‚úÖ Model {seed+1}/{n_models} complete - Best SMAPE: {best_val_smape:.2f}%\")\n","\n","    # Ensemble validation\n","    print(f\"\\nüìä Ensemble Validation for {bin_name}:\")\n","    all_preds = []\n","    for model in models:\n","        model.eval()\n","        with torch.no_grad():\n","            pred = model(X_val)\n","            pred_clipped = torch.clamp(pred,\n","                                      min=np.log1p(3),\n","                                      max=np.log1p(500))\n","            all_preds.append(pred_clipped.cpu())\n","\n","    # Average predictions\n","    ensemble_pred = torch.stack(all_preds).mean(dim=0)\n","    ensemble_pred_orig = np.expm1(ensemble_pred.numpy().squeeze())\n","\n","    # Calculate both metrics\n","    ensemble_smape = smape_metric(y_val.cpu().numpy().squeeze(),\n","                                 ensemble_pred.numpy().squeeze())\n","    ensemble_mae = mean_absolute_error(val_true_orig, ensemble_pred_orig)\n","\n","    print(f\"   Individual models SMAPE range: {min(val_smapes):.2f}% - {max(val_smapes):.2f}%\")\n","    print(f\"   Individual models SMAPE mean: {np.mean(val_smapes):.2f}%\")\n","    print(f\"   üèÜ Ensemble SMAPE: {ensemble_smape:.2f}%\")\n","    print(f\"   üìä Ensemble MAE: ${ensemble_mae:.2f}\")\n","\n","    return models, ensemble_smape, val_smapes  # Return SMAPE instead of MAE\n","\n"],"metadata":{"id":"Fg6XTlP_vXjc","executionInfo":{"status":"ok","timestamp":1760289846139,"user_tz":-330,"elapsed":10,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["\n","# =========================================================================\n","# PHASE 1: TRAIN ALL SPECIALISTS\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèãÔ∏è PHASE 1: TRAINING SPECIALIST ENSEMBLES\")\n","print(\"=\"*70)\n","\n","# Store all results\n","all_specialists = {}\n","all_results = {}\n","\n","# -------------------------------------------------------------------------\n","# 1. Train AFFORDABLE Specialist (<$50)\n","# -------------------------------------------------------------------------\n","if train_affordable_input.shape[0] > 1000:  # Only if enough data\n","    affordable_models, affordable_smape, affordable_smapes = train_ensemble(\n","        X_train=train_affordable_input,\n","        y_train=train_affordable_targets,\n","        X_val=val_affordable_input,\n","        y_val=val_affordable_targets,\n","        n_models=10,  # Plenty of data\n","        bin_name=\"Affordable\",\n","        epochs=30,\n","        lr=1e-3,\n","        use_weighted=True\n","    )\n","    all_specialists['affordable'] = affordable_models\n","    all_results['affordable'] = affordable_smape\n","else:\n","    print(\"\\n‚ö†Ô∏è  Too few affordable samples, skipping...\")\n","    all_specialists['affordable'] = None\n","\n","# -------------------------------------------------------------------------\n","# 2. Train MID-RANGE Specialist ($50-$100)\n","# -------------------------------------------------------------------------\n","if train_mid_input.shape[0] > 500:  # Only if enough data\n","    mid_models, mid_mae, mid_smapes  = train_ensemble(\n","        X_train=train_mid_input,\n","        y_train=train_mid_targets,\n","        X_val=val_mid_input,\n","        y_val=val_mid_targets,\n","        n_models=7,  # Less data, fewer models\n","        bin_name=\"Mid-range\",\n","        epochs=40,  # More epochs, less data\n","        lr=5e-4,  # Lower learning rate\n","        use_weighted=False\n","    )\n","    all_specialists['mid'] = mid_models\n","    all_results['mid'] = mid_mae\n","else:\n","    print(\"\\n‚ö†Ô∏è  Too few mid-range samples, skipping...\")\n","    all_specialists['mid'] = None\n","\n","# -------------------------------------------------------------------------\n","# 3. Train PREMIUM Specialist ($85+)  # Updated threshold!\n","# -------------------------------------------------------------------------\n","if train_premium_input.shape[0] > 300:\n","    # Define regularized model for small dataset\n","    class RegularizedMLP(nn.Module):\n","        def __init__(self, input_dim):\n","            super().__init__()\n","            self.model = nn.Sequential(\n","                nn.Linear(input_dim, 1024),\n","                nn.ReLU(),\n","                nn.BatchNorm1d(1024),\n","                nn.Dropout(0.4),\n","\n","                nn.Linear(1024, 512),\n","                nn.ReLU(),\n","                nn.BatchNorm1d(512),\n","                nn.Dropout(0.4),\n","\n","                nn.Linear(512, 256),\n","                nn.ReLU(),\n","                nn.BatchNorm1d(256),\n","                nn.Dropout(0.3),\n","\n","                nn.Linear(256, 1)\n","            )\n","\n","        def forward(self, x):\n","            return self.model(x)\n","\n","    # Pass the custom model class directly\n","    premium_models, premium_smape, premium_smapes = train_ensemble(\n","        X_train=train_premium_input,\n","        y_train=train_premium_targets,\n","        X_val=val_premium_input,\n","        y_val=val_premium_targets,\n","        n_models=5,\n","        bin_name=\"Premium\",\n","        epochs=60,\n","        lr=2e-4,\n","        use_weighted=True,\n","        model_class=RegularizedMLP\n","    )\n","\n","# Sort models by SMAPE and keep only top 3\n","sorted_pairs = sorted(zip(premium_smapes, premium_models))\n","top_3_models = [m for _, m in sorted_pairs[:3]]\n","top_3_smapes = [s for s, _ in sorted_pairs[:3]]\n","\n","print(f\"\\nüîç Premium Model Selection:\")\n","print(f\"   Best 3 models SMAPE: {top_3_smapes[0]:.2f}%, {top_3_smapes[1]:.2f}%, {top_3_smapes[2]:.2f}%\")\n","print(f\"   Discarded 2 models SMAPE: {sorted_pairs[3][0]:.2f}%, {sorted_pairs[4][0]:.2f}%\")\n","\n","# Save only top 3\n","all_specialists['premium'] = top_3_models\n","all_results['premium'] = premium_smape  # Ensemble SMAPE for reference\n","\n","# =========================================================================\n","# SUMMARY\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìä TRAINING SUMMARY\")\n","print(\"=\"*70)\n","\n","total_models = sum(len(models) for models in all_specialists.values() if models is not None)\n","print(f\"\\n‚úÖ Total models trained: {total_models}\")\n","\n","print(f\"\\nüèÜ Validation Results by Bin:\")\n","for bin_name, mae in all_results.items():\n","    print(f\"   {bin_name.capitalize():12} MAE: ${mae:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wum2VCwEptvY","executionInfo":{"status":"ok","timestamp":1760290874766,"user_tz":-330,"elapsed":369392,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"a9c43e7b-f615-4486-f8f2-96a7d7d845eb"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üèãÔ∏è PHASE 1: TRAINING SPECIALIST ENSEMBLES\n","======================================================================\n","\n","======================================================================\n","üöÄ Training AFFORDABLE Ensemble (10 models)\n","   Loss: Weighted SMAPE\n","======================================================================\n","Training samples: 50903\n","Validation samples: 12743\n","\n","üîÑ Training model 1/10 (seed=0)...\n","   Epoch 10/30: Loss = 0.2100, Val SMAPE = 50.99%, Val MAE = $17.82\n","   Epoch 20/30: Loss = 0.1837, Val SMAPE = 50.46%, Val MAE = $26.07\n","   Epoch 30/30: Loss = 0.1688, Val SMAPE = 49.94%, Val MAE = $6.97\n","‚úÖ Model 1/10 complete - Best SMAPE: 49.94%\n","\n","üîÑ Training model 2/10 (seed=1)...\n","   Epoch 10/30: Loss = 0.2106, Val SMAPE = 50.74%, Val MAE = $6.40\n","   Epoch 20/30: Loss = 0.1883, Val SMAPE = 51.09%, Val MAE = $19.57\n","   Epoch 30/30: Loss = 0.1712, Val SMAPE = 50.34%, Val MAE = $6.81\n","‚úÖ Model 2/10 complete - Best SMAPE: 50.34%\n","\n","üîÑ Training model 3/10 (seed=2)...\n","   Epoch 10/30: Loss = 0.2102, Val SMAPE = 52.01%, Val MAE = $26.32\n","   Epoch 20/30: Loss = 0.1858, Val SMAPE = 51.02%, Val MAE = $11.04\n","   Epoch 30/30: Loss = 0.1692, Val SMAPE = 49.20%, Val MAE = $6.25\n","‚úÖ Model 3/10 complete - Best SMAPE: 49.20%\n","\n","üîÑ Training model 4/10 (seed=3)...\n","   Epoch 10/30: Loss = 0.2232, Val SMAPE = 51.46%, Val MAE = $6.52\n","   Epoch 20/30: Loss = 0.1946, Val SMAPE = 49.98%, Val MAE = $6.38\n","   Epoch 30/30: Loss = 0.1741, Val SMAPE = 48.95%, Val MAE = $6.26\n","‚úÖ Model 4/10 complete - Best SMAPE: 48.95%\n","\n","üîÑ Training model 5/10 (seed=4)...\n","   Epoch 10/30: Loss = 0.2141, Val SMAPE = 50.77%, Val MAE = $6.91\n","   Epoch 20/30: Loss = 0.2017, Val SMAPE = 52.97%, Val MAE = $10.29\n","   Epoch 30/30: Loss = 0.1749, Val SMAPE = 50.17%, Val MAE = $6.65\n","‚úÖ Model 5/10 complete - Best SMAPE: 50.17%\n","\n","üîÑ Training model 6/10 (seed=5)...\n","   Epoch 10/30: Loss = 0.2186, Val SMAPE = 51.15%, Val MAE = $6.50\n","   Epoch 20/30: Loss = 0.1894, Val SMAPE = 50.31%, Val MAE = $6.63\n","   Epoch 30/30: Loss = 0.1738, Val SMAPE = 49.63%, Val MAE = $6.34\n","‚úÖ Model 6/10 complete - Best SMAPE: 49.63%\n","\n","üîÑ Training model 7/10 (seed=6)...\n","   Epoch 10/30: Loss = 0.2070, Val SMAPE = 51.25%, Val MAE = $26.00\n","   Epoch 20/30: Loss = 0.1845, Val SMAPE = 50.44%, Val MAE = $26.13\n","   Epoch 30/30: Loss = 0.1676, Val SMAPE = 50.92%, Val MAE = $7.35\n","‚úÖ Model 7/10 complete - Best SMAPE: 50.44%\n","\n","üîÑ Training model 8/10 (seed=7)...\n","   Epoch 10/30: Loss = 0.2113, Val SMAPE = 51.23%, Val MAE = $6.78\n","   Epoch 20/30: Loss = 0.1921, Val SMAPE = 50.01%, Val MAE = $8.00\n","   Epoch 30/30: Loss = 0.1717, Val SMAPE = 49.07%, Val MAE = $6.24\n","‚úÖ Model 8/10 complete - Best SMAPE: 49.07%\n","\n","üîÑ Training model 9/10 (seed=8)...\n","   Epoch 10/30: Loss = 0.2074, Val SMAPE = 51.95%, Val MAE = $26.28\n","   Epoch 20/30: Loss = 0.1868, Val SMAPE = 49.56%, Val MAE = $6.30\n","   Epoch 30/30: Loss = 0.1675, Val SMAPE = 50.40%, Val MAE = $16.13\n","‚úÖ Model 9/10 complete - Best SMAPE: 49.56%\n","\n","üîÑ Training model 10/10 (seed=9)...\n","   Epoch 10/30: Loss = 0.2116, Val SMAPE = 50.91%, Val MAE = $6.47\n","   Epoch 20/30: Loss = 0.1916, Val SMAPE = 51.60%, Val MAE = $21.25\n","   Epoch 30/30: Loss = 0.1717, Val SMAPE = 49.77%, Val MAE = $6.44\n","‚úÖ Model 10/10 complete - Best SMAPE: 49.77%\n","\n","üìä Ensemble Validation for Affordable:\n","   Individual models SMAPE range: 48.95% - 50.44%\n","   Individual models SMAPE mean: 49.71%\n","   üèÜ Ensemble SMAPE: 47.74%\n","   üìä Ensemble MAE: $6.09\n","\n","======================================================================\n","üöÄ Training MID-RANGE Ensemble (7 models)\n","   Loss: SMAPE\n","======================================================================\n","Training samples: 6056\n","Validation samples: 1494\n","\n","üîÑ Training model 1/7 (seed=0)...\n","   Epoch 10/40: Loss = 0.2669, Val SMAPE = 34.41%, Val MAE = $13.78\n","   Epoch 20/40: Loss = 0.1551, Val SMAPE = 24.21%, Val MAE = $11.03\n","   Epoch 30/40: Loss = 0.1094, Val SMAPE = 19.25%, Val MAE = $9.45\n","   Epoch 40/40: Loss = 0.0940, Val SMAPE = 23.30%, Val MAE = $11.49\n","‚úÖ Model 1/7 complete - Best SMAPE: 19.25%\n","\n","üîÑ Training model 2/7 (seed=1)...\n","   Epoch 10/40: Loss = 0.3601, Val SMAPE = 41.95%, Val MAE = $16.52\n","   Epoch 20/40: Loss = 0.2129, Val SMAPE = 25.27%, Val MAE = $11.64\n","   Epoch 30/40: Loss = 0.1389, Val SMAPE = 22.65%, Val MAE = $11.22\n","   Epoch 40/40: Loss = 0.0968, Val SMAPE = 21.32%, Val MAE = $10.78\n","‚úÖ Model 2/7 complete - Best SMAPE: 21.32%\n","\n","üîÑ Training model 3/7 (seed=2)...\n","   Epoch 10/40: Loss = 0.2596, Val SMAPE = 30.12%, Val MAE = $13.03\n","   Epoch 20/40: Loss = 0.1495, Val SMAPE = 20.84%, Val MAE = $9.86\n","   Epoch 30/40: Loss = 0.1031, Val SMAPE = 27.24%, Val MAE = $13.16\n","   Epoch 40/40: Loss = 0.1011, Val SMAPE = 24.54%, Val MAE = $12.62\n","‚úÖ Model 3/7 complete - Best SMAPE: 20.84%\n","\n","üîÑ Training model 4/7 (seed=3)...\n","   Epoch 10/40: Loss = 0.3184, Val SMAPE = 37.73%, Val MAE = $16.14\n","   Epoch 20/40: Loss = 0.1814, Val SMAPE = 23.62%, Val MAE = $10.62\n","   Epoch 30/40: Loss = 0.1474, Val SMAPE = 23.97%, Val MAE = $11.05\n","   Epoch 40/40: Loss = 0.1094, Val SMAPE = 20.87%, Val MAE = $10.73\n","‚úÖ Model 4/7 complete - Best SMAPE: 20.87%\n","\n","üîÑ Training model 5/7 (seed=4)...\n","   Epoch 10/40: Loss = 0.2341, Val SMAPE = 31.28%, Val MAE = $13.87\n","   Epoch 20/40: Loss = 0.1634, Val SMAPE = 25.29%, Val MAE = $11.74\n","   Epoch 30/40: Loss = 0.1089, Val SMAPE = 18.02%, Val MAE = $9.05\n","   Epoch 40/40: Loss = 0.0994, Val SMAPE = 26.06%, Val MAE = $12.90\n","‚úÖ Model 5/7 complete - Best SMAPE: 18.02%\n","\n","üîÑ Training model 6/7 (seed=5)...\n","   Epoch 10/40: Loss = 0.3988, Val SMAPE = 41.80%, Val MAE = $15.45\n","   Epoch 20/40: Loss = 0.3182, Val SMAPE = 34.83%, Val MAE = $13.51\n","   Epoch 30/40: Loss = 0.2173, Val SMAPE = 25.95%, Val MAE = $10.94\n","   Epoch 40/40: Loss = 0.1862, Val SMAPE = 20.81%, Val MAE = $9.77\n","‚úÖ Model 6/7 complete - Best SMAPE: 20.81%\n","\n","üîÑ Training model 7/7 (seed=6)...\n","   Epoch 10/40: Loss = 0.3374, Val SMAPE = 35.03%, Val MAE = $13.63\n","   Epoch 20/40: Loss = 0.2917, Val SMAPE = 30.08%, Val MAE = $12.20\n","   Epoch 30/40: Loss = 0.1904, Val SMAPE = 20.98%, Val MAE = $9.82\n","   Epoch 40/40: Loss = 0.1380, Val SMAPE = 18.38%, Val MAE = $9.13\n","‚úÖ Model 7/7 complete - Best SMAPE: 18.38%\n","\n","üìä Ensemble Validation for Mid-range:\n","   Individual models SMAPE range: 18.02% - 21.32%\n","   Individual models SMAPE mean: 19.93%\n","   üèÜ Ensemble SMAPE: 20.59%\n","   üìä Ensemble MAE: $10.54\n","\n","======================================================================\n","üöÄ Training PREMIUM Ensemble (5 models)\n","   Loss: Weighted SMAPE\n","======================================================================\n","Training samples: 3041\n","Validation samples: 763\n","\n","üîÑ Training model 1/5 (seed=0)...\n","   Epoch 10/60: Loss = 2.8016, Val SMAPE = 85.50%, Val MAE = $78.38\n","   Epoch 20/60: Loss = 2.1328, Val SMAPE = 69.39%, Val MAE = $67.87\n","   Epoch 30/60: Loss = 1.9083, Val SMAPE = 60.44%, Val MAE = $63.22\n","   Epoch 40/60: Loss = 1.3881, Val SMAPE = 48.19%, Val MAE = $56.62\n","   Epoch 50/60: Loss = 1.3961, Val SMAPE = 45.86%, Val MAE = $54.27\n","   Epoch 60/60: Loss = 1.3217, Val SMAPE = 44.13%, Val MAE = $52.78\n","‚úÖ Model 1/5 complete - Best SMAPE: 44.13%\n","\n","üîÑ Training model 2/5 (seed=1)...\n","   Epoch 10/60: Loss = 2.5756, Val SMAPE = 81.34%, Val MAE = $83.49\n","   Epoch 20/60: Loss = 1.7307, Val SMAPE = 60.77%, Val MAE = $63.96\n","   Epoch 30/60: Loss = 1.3359, Val SMAPE = 53.51%, Val MAE = $58.91\n","   Epoch 40/60: Loss = 1.3104, Val SMAPE = 48.88%, Val MAE = $53.92\n","   Epoch 50/60: Loss = 1.2973, Val SMAPE = 48.48%, Val MAE = $53.52\n","   Epoch 60/60: Loss = 1.1174, Val SMAPE = 43.85%, Val MAE = $51.74\n","‚úÖ Model 2/5 complete - Best SMAPE: 43.85%\n","\n","üîÑ Training model 3/5 (seed=2)...\n","   Epoch 10/60: Loss = 2.6179, Val SMAPE = 71.54%, Val MAE = $72.34\n","   Epoch 20/60: Loss = 1.8769, Val SMAPE = 58.38%, Val MAE = $62.97\n","   Epoch 30/60: Loss = 1.6600, Val SMAPE = 53.74%, Val MAE = $61.07\n","   Epoch 40/60: Loss = 1.4770, Val SMAPE = 49.65%, Val MAE = $58.82\n","   Epoch 50/60: Loss = 1.1525, Val SMAPE = 40.64%, Val MAE = $52.69\n","   Epoch 60/60: Loss = 1.0932, Val SMAPE = 38.15%, Val MAE = $51.08\n","‚úÖ Model 3/5 complete - Best SMAPE: 38.15%\n","\n","üîÑ Training model 4/5 (seed=3)...\n","   Epoch 10/60: Loss = 2.7840, Val SMAPE = 86.96%, Val MAE = $79.82\n","   Epoch 20/60: Loss = 2.1847, Val SMAPE = 67.40%, Val MAE = $67.92\n","   Epoch 30/60: Loss = 1.7927, Val SMAPE = 60.24%, Val MAE = $67.29\n","   Epoch 40/60: Loss = 1.4349, Val SMAPE = 49.85%, Val MAE = $58.26\n","   Epoch 50/60: Loss = 1.2289, Val SMAPE = 48.64%, Val MAE = $60.97\n","   Epoch 60/60: Loss = 1.0299, Val SMAPE = 44.04%, Val MAE = $56.70\n","‚úÖ Model 4/5 complete - Best SMAPE: 44.04%\n","\n","üîÑ Training model 5/5 (seed=4)...\n","   Epoch 10/60: Loss = 2.6526, Val SMAPE = 72.89%, Val MAE = $64.35\n","   Epoch 20/60: Loss = 2.2791, Val SMAPE = 67.13%, Val MAE = $62.53\n","   Epoch 30/60: Loss = 2.0014, Val SMAPE = 64.09%, Val MAE = $65.89\n","   Epoch 40/60: Loss = 1.7019, Val SMAPE = 52.90%, Val MAE = $59.46\n","   Epoch 50/60: Loss = 1.3813, Val SMAPE = 46.59%, Val MAE = $55.45\n","   Epoch 60/60: Loss = 1.3120, Val SMAPE = 49.16%, Val MAE = $59.33\n","‚úÖ Model 5/5 complete - Best SMAPE: 46.59%\n","\n","üìä Ensemble Validation for Premium:\n","   Individual models SMAPE range: 38.15% - 46.59%\n","   Individual models SMAPE mean: 43.35%\n","   üèÜ Ensemble SMAPE: 44.26%\n","   üìä Ensemble MAE: $52.33\n","\n","üîç Premium Model Selection:\n","   Best 3 models SMAPE: 38.15%, 43.85%, 44.04%\n","   Discarded 2 models SMAPE: 44.13%, 46.59%\n","\n","======================================================================\n","üìä TRAINING SUMMARY\n","======================================================================\n","\n","‚úÖ Total models trained: 20\n","\n","üèÜ Validation Results by Bin:\n","   Affordable   MAE: $47.74\n","   Mid          MAE: $20.59\n","   Premium      MAE: $44.26\n"]}]},{"cell_type":"code","source":["# =========================================================================\n","# SAVE MODELS (No selection needed - already done!)\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üíæ SAVING SPECIALIST MODELS\")\n","print(\"=\"*70)\n","\n","save_dir = '/content/drive/MyDrive/amazon_ml_challenge/price_specialists'\n","os.makedirs(save_dir, exist_ok=True)\n","\n","for bin_name, models in all_specialists.items():\n","    if models is not None:\n","        bin_dir = os.path.join(save_dir, bin_name)\n","        os.makedirs(bin_dir, exist_ok=True)\n","\n","        for i, model in enumerate(models):\n","            model_path = os.path.join(bin_dir, f'model_{i}.pt')\n","            torch.save(model.state_dict(), model_path)\n","\n","        print(f\"‚úÖ Saved {len(models)} models for {bin_name} ‚Üí {bin_dir}\")\n","\n","# Save metadata\n","metadata = {\n","    'bin_boundaries': BIN_BOUNDARIES,\n","    'results': all_results,\n","    'input_dim': train_input.shape[1],\n","    'n_models': {k: len(v) if v else 0 for k, v in all_specialists.items()},\n","    'train_distribution': {\n","        'affordable': train_affordable_mask.sum(),\n","        'mid': train_mid_mask.sum(),\n","        'premium': train_premium_mask.sum()\n","    },\n","    'metric': 'SMAPE'\n","}\n","\n","metadata_path = os.path.join(save_dir, 'metadata.pt')\n","torch.save(metadata, metadata_path)\n","print(f\"‚úÖ Saved metadata ‚Üí {metadata_path}\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ ALL MODELS SAVED!\")\n","print(\"=\"*70)\n","print(f\"   Affordable: 10 models\")\n","print(f\"   Mid-range: 7 models\")\n","print(f\"   Premium: 3 models (top performers only)\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B3uifC8bwBqn","executionInfo":{"status":"ok","timestamp":1760290875953,"user_tz":-330,"elapsed":1185,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"95310ecb-e0cb-4770-c986-2c98afe156e0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üíæ SAVING SPECIALIST MODELS\n","======================================================================\n","‚úÖ Saved 10 models for affordable ‚Üí /content/drive/MyDrive/amazon_ml_challenge/price_specialists/affordable\n","‚úÖ Saved 7 models for mid ‚Üí /content/drive/MyDrive/amazon_ml_challenge/price_specialists/mid\n","‚úÖ Saved 3 models for premium ‚Üí /content/drive/MyDrive/amazon_ml_challenge/price_specialists/premium\n","‚úÖ Saved metadata ‚Üí /content/drive/MyDrive/amazon_ml_challenge/price_specialists/metadata.pt\n","\n","======================================================================\n","‚úÖ ALL MODELS SAVED!\n","======================================================================\n","   Affordable: 10 models\n","   Mid-range: 7 models\n","   Premium: 3 models (top performers only)\n"]}]},{"cell_type":"code","source":["# =========================================================================\n","# PRICE-BASED SPECIALIST MODELS - PHASE 2 & 3 (SMAPE VERSION)\n","# Phase 2: Validation (compare specialists vs baseline)\n","# Phase 3: Test Set Predictions\n","# =========================================================================\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","import pandas as pd\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import mean_absolute_error\n","from tqdm import tqdm\n","import os\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Using device: {device}\")\n","\n","# =========================================================================\n","# SMAPE METRIC\n","# =========================================================================\n","def smape_metric(y_true, y_pred):\n","    \"\"\"Calculate SMAPE in original scale\"\"\"\n","    return np.mean(np.abs(y_true - y_pred) / ((np.abs(y_true) + np.abs(y_pred)) / 2 + 1e-8)) * 100\n","\n","# =========================================================================\n","# MODEL DEFINITION (Same as training)\n","# =========================================================================\n","class RegressionMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# =========================================================================\n","# LOAD SPECIALIST MODELS\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìÇ LOADING SPECIALIST MODELS\")\n","print(\"=\"*70)\n","\n","save_dir = '/content/drive/MyDrive/amazon_ml_challenge/price_specialists'\n","metadata_path = os.path.join(save_dir, 'metadata.pt')\n","\n","# Load metadata\n","metadata = torch.load(metadata_path, weights_only=False)\n","input_dim = metadata['input_dim']\n","bin_boundaries = metadata['bin_boundaries']\n","\n","print(f\"‚úÖ Metadata loaded\")\n","print(f\"   Input dimension: {input_dim}\")\n","print(f\"   Bins: {list(bin_boundaries.keys())}\")\n","\n","# Load all specialist models\n","specialists = {}\n","\n","for bin_name in ['affordable', 'mid', 'premium']:\n","    bin_dir = os.path.join(save_dir, bin_name)\n","\n","    if os.path.exists(bin_dir):\n","        models = []\n","        model_files = sorted([f for f in os.listdir(bin_dir) if f.endswith('.pt')])\n","\n","        for model_file in model_files:\n","            model = RegressionMLP(input_dim).to(device)\n","            model.load_state_dict(torch.load(\n","                os.path.join(bin_dir, model_file),\n","                map_location=device\n","            ))\n","            model.eval()\n","            models.append(model)\n","\n","        specialists[bin_name] = models\n","        print(f\"‚úÖ Loaded {len(models)} models for {bin_name}\")\n","    else:\n","        specialists[bin_name] = None\n","        print(f\"‚ö†Ô∏è  No models found for {bin_name}\")\n","\n","# =========================================================================\n","# LOAD BASELINE ENSEMBLE (for routing)\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìÇ LOADING BASELINE ENSEMBLE (for routing)\")\n","print(\"=\"*70)\n","\n","baseline_dir = '/content/drive/MyDrive/amazon_ml_challenge/ensemble_CLIP_models'\n","baseline_models = []\n","\n","for i in range(10):\n","    model_path = os.path.join(baseline_dir, f'clip_ensemble_model_{i}.pt')\n","    if os.path.exists(model_path):\n","        model = RegressionMLP(input_dim).to(device)\n","        model.load_state_dict(torch.load(model_path, map_location=device))\n","        model.eval()\n","        baseline_models.append(model)\n","\n","print(f\"‚úÖ Loaded {len(baseline_models)} baseline models\")\n","\n","# =========================================================================\n","# HELPER FUNCTIONS (UPDATED WITH NEW CLIPPING)\n","# =========================================================================\n","def ensemble_predict(models, x, use_clipping=True, top_k=None):\n","    \"\"\"Average predictions with optional selective ensemble\"\"\"\n","    predictions = []\n","\n","    min_log_value = np.log1p(3)\n","    max_log_value = np.log1p(3000)\n","\n","    if x.ndim == 1:\n","        x = x.unsqueeze(0)\n","\n","    # Use only top K models if specified\n","    models_to_use = models[:top_k] if top_k is not None else models\n","\n","    for model in models_to_use:\n","        model.eval()\n","        with torch.no_grad():\n","            pred = model(x)\n","            if use_clipping:\n","                pred = torch.clamp(pred, min=min_log_value, max=max_log_value)\n","            predictions.append(pred)\n","\n","    return torch.stack(predictions).mean(dim=0)\n","\n","def get_price_bin(price):\n","    \"\"\"Map price to bin index - UPDATED THRESHOLDS\"\"\"\n","    if price < 40:  # Changed from 50\n","        return 0  # affordable\n","    elif price < 85:  # Changed from 100\n","        return 1  # mid\n","    else:\n","        return 2  # premium\n","\n","def predict_with_routing(x, baseline_models, specialists):\n","    \"\"\"\n","    Predict using routing strategy:\n","    1. Use baseline to predict rough price\n","    2. Route to appropriate specialist\n","    3. Get final prediction from specialist\n","    \"\"\"\n","    # Step 1: Get rough price from baseline (for routing)\n","    baseline_pred_log = ensemble_predict(baseline_models, x)\n","    baseline_pred_orig = np.expm1(baseline_pred_log.cpu().numpy().squeeze())\n","\n","    # Handle single sample vs batch\n","    if baseline_pred_orig.ndim == 0:\n","        baseline_pred_orig = np.array([baseline_pred_orig])\n","\n","    # Step 2: Route to bins\n","    bin_indices = np.array([get_price_bin(p) for p in baseline_pred_orig])\n","\n","    # Step 3: Predict with specialists\n","    final_preds = []\n","    bin_names = ['affordable', 'mid', 'premium']\n","\n","    for i, (sample, bin_idx) in enumerate(zip(x, bin_indices)):\n","        bin_name = bin_names[bin_idx]\n","\n","        # Use specialist if available, else use baseline\n","        if specialists[bin_name] is not None:\n","          # Use only top 3 models for premium, all for others\n","          k = 3 if bin_name == 'premium' else None\n","          pred = ensemble_predict(specialists[bin_name], sample.unsqueeze(0), top_k=k)\n","\n","        else:\n","            pred = baseline_pred_log[i].unsqueeze(0)\n","\n","        final_preds.append(pred)\n","\n","    return torch.cat(final_preds)\n","\n","# =========================================================================\n","# PHASE 2: VALIDATION\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üß™ PHASE 2: VALIDATION - SPECIALISTS vs BASELINE\")\n","print(\"=\"*70)\n","\n","val_input = val_input.to(device)\n","val_targets = val_targets.to(device)\n","val_prices_orig = np.expm1(val_targets.cpu().numpy().squeeze())\n","\n","# Split validation by true bins (UPDATED THRESHOLDS)\n","val_affordable_mask = val_prices_orig < 40  # Changed from 50\n","val_mid_mask = (val_prices_orig >= 40) & (val_prices_orig < 85)  # Changed from 50-100\n","val_premium_mask = val_prices_orig >= 85  # Changed from 100\n","\n","print(f\"\\nüìä Validation Data:\")\n","print(f\"  Total samples: {len(val_prices_orig)}\")\n","print(f\"  Affordable (<$40): {val_affordable_mask.sum()} ({val_affordable_mask.sum()/len(val_prices_orig)*100:.1f}%)\")\n","print(f\"  Mid-range ($40-$85): {val_mid_mask.sum()} ({val_mid_mask.sum()/len(val_prices_orig)*100:.1f}%)\")\n","print(f\"  Premium (>$85): {val_premium_mask.sum()} ({val_premium_mask.sum()/len(val_prices_orig)*100:.1f}%)\")\n","\n","# -------------------------------------------------------------------------\n","# Method 1: BASELINE (current approach)\n","# -------------------------------------------------------------------------\n","print(\"\\n\" + \"-\"*70)\n","print(\"üìä Method 1: BASELINE ENSEMBLE\")\n","print(\"-\"*70)\n","\n","baseline_preds = []\n","batch_size = 512\n","\n","val_loader = DataLoader(\n","    TensorDataset(val_input),\n","    batch_size=batch_size,\n","    shuffle=False\n",")\n","\n","for (batch_x,) in tqdm(val_loader, desc=\"Baseline predictions\"):\n","    if batch_x.ndim == 1:\n","        batch_x = batch_x.unsqueeze(0)\n","    pred = ensemble_predict(baseline_models, batch_x.to(device))\n","    baseline_preds.append(pred.cpu())\n","\n","baseline_preds = torch.cat(baseline_preds)\n","baseline_preds_orig = np.expm1(baseline_preds.numpy().squeeze())\n","\n","# Calculate both metrics\n","baseline_mae = mean_absolute_error(val_prices_orig, baseline_preds_orig)\n","baseline_smape = smape_metric(val_prices_orig, baseline_preds_orig)\n","\n","print(f\"\\nüéØ Baseline Overall SMAPE: {baseline_smape:.2f}%\")\n","print(f\"üìä Baseline Overall MAE: ${baseline_mae:.2f}\")\n","\n","# Per-bin analysis for baseline\n","print(f\"\\nüìä Baseline Performance by Price Range:\")\n","for bin_name, mask in [('Affordable', val_affordable_mask),\n","                        ('Mid-range', val_mid_mask),\n","                        ('Premium', val_premium_mask)]:\n","    if mask.sum() > 0:\n","        bin_smape = smape_metric(val_prices_orig[mask], baseline_preds_orig[mask])\n","        bin_mae = mean_absolute_error(val_prices_orig[mask], baseline_preds_orig[mask])\n","        print(f\"  {bin_name:12}: {bin_smape:6.2f}% SMAPE, ${bin_mae:7.2f} MAE ({mask.sum()} samples)\")\n","\n","# -------------------------------------------------------------------------\n","# Method 2: SPECIALIST ENSEMBLES with Routing\n","# -------------------------------------------------------------------------\n","print(\"\\n\" + \"-\"*70)\n","print(\"üìä Method 2: SPECIALIST ENSEMBLES with Routing\")\n","print(\"-\"*70)\n","\n","specialist_preds = []\n","\n","for (batch_x,) in tqdm(val_loader, desc=\"Specialist predictions\"):\n","    if batch_x.ndim == 1:\n","        batch_x = batch_x.unsqueeze(0)\n","    pred = predict_with_routing(batch_x.to(device), baseline_models, specialists)\n","    specialist_preds.append(pred.cpu())\n","\n","specialist_preds = torch.cat(specialist_preds)\n","specialist_preds_orig = np.expm1(specialist_preds.numpy().squeeze())\n","\n","# Calculate both metrics\n","specialist_mae = mean_absolute_error(val_prices_orig, specialist_preds_orig)\n","specialist_smape = smape_metric(val_prices_orig, specialist_preds_orig)\n","\n","print(f\"\\nüéØ Specialist Overall SMAPE: {specialist_smape:.2f}%\")\n","print(f\"üìä Specialist Overall MAE: ${specialist_mae:.2f}\")\n","\n","# Per-bin analysis for specialists\n","print(f\"\\nüìä Specialist Performance by Price Range:\")\n","for bin_name, mask in [('Affordable', val_affordable_mask),\n","                        ('Mid-range', val_mid_mask),\n","                        ('Premium', val_premium_mask)]:\n","    if mask.sum() > 0:\n","        bin_smape = smape_metric(val_prices_orig[mask], specialist_preds_orig[mask])\n","        bin_mae = mean_absolute_error(val_prices_orig[mask], specialist_preds_orig[mask])\n","        print(f\"  {bin_name:12}: {bin_smape:6.2f}% SMAPE, ${bin_mae:7.2f} MAE ({mask.sum()} samples)\")\n","\n","# -------------------------------------------------------------------------\n","# COMPARISON (SMAPE IS PRIMARY METRIC)\n","# -------------------------------------------------------------------------\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèÜ VALIDATION RESULTS COMPARISON\")\n","print(\"=\"*70)\n","\n","improvement_smape = baseline_smape - specialist_smape\n","improvement_pct = (improvement_smape / baseline_smape) * 100\n","\n","print(f\"\\n{'Method':<30} {'SMAPE':>12} {'MAE':>12}\")\n","print(\"-\"*70)\n","print(f\"{'Baseline Ensemble':<30} {baseline_smape:>11.2f}% ${baseline_mae:>10.2f}\")\n","print(f\"{'Specialist Ensembles':<30} {specialist_smape:>11.2f}% ${specialist_mae:>10.2f}\")\n","print(\"-\"*70)\n","print(f\"{'SMAPE Improvement':<30} {improvement_smape:>11.2f}% ({improvement_pct:+.2f}%)\")\n","\n","if specialist_smape < baseline_smape:\n","    print(f\"\\nüéâ SUCCESS! Specialists improve SMAPE performance!\")\n","    print(f\"‚úÖ Use specialists for test predictions\")\n","    use_specialists = True\n","else:\n","    print(f\"\\n‚ö†Ô∏è  Specialists don't improve SMAPE over baseline\")\n","    print(f\"üí° Consider: Use baseline for test, or retrain with more premium focus\")\n","    use_specialists = False\n","\n","# Routing accuracy\n","baseline_bins = np.array([get_price_bin(p) for p in baseline_preds_orig])\n","true_bins = np.array([get_price_bin(p) for p in val_prices_orig])\n","routing_accuracy = (baseline_bins == true_bins).mean()\n","\n","print(f\"\\nüéØ Routing Accuracy: {routing_accuracy*100:.1f}%\")\n","print(f\"   (How often baseline correctly predicts price range)\")\n","\n","# =========================================================================\n","# PREMIUM PREDICTION ANALYSIS (MOST IMPORTANT FOR SMAPE)\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìà PREMIUM PREDICTION ANALYSIS (Critical for SMAPE)\")\n","print(\"=\"*70)\n","\n","premium_mask = val_premium_mask\n","\n","if premium_mask.sum() > 0:\n","    premium_preds_spec = specialist_preds_orig[premium_mask]\n","    premium_preds_base = baseline_preds_orig[premium_mask]\n","    premium_actual = val_prices_orig[premium_mask]\n","\n","    print(f\"\\nüìä Premium Item Analysis ({premium_mask.sum()} samples, >${85}):\")\n","    print(f\"  Actual mean:              ${premium_actual.mean():.2f}\")\n","    print(f\"  Specialist predictions:   ${premium_preds_spec.mean():.2f}\")\n","    print(f\"  Baseline predictions:     ${premium_preds_base.mean():.2f}\")\n","\n","    premium_smape_spec = smape_metric(premium_actual, premium_preds_spec)\n","    premium_smape_base = smape_metric(premium_actual, premium_preds_base)\n","    premium_mae_spec = mean_absolute_error(premium_actual, premium_preds_spec)\n","    premium_mae_base = mean_absolute_error(premium_actual, premium_preds_base)\n","\n","    print(f\"\\n  Specialist Premium SMAPE: {premium_smape_spec:.2f}%\")\n","    print(f\"  Baseline Premium SMAPE:   {premium_smape_base:.2f}%\")\n","    print(f\"  Specialist Premium MAE:   ${premium_mae_spec:.2f}\")\n","    print(f\"  Baseline Premium MAE:     ${premium_mae_base:.2f}\")\n","\n","    # Diagnosis\n","    if premium_preds_spec.mean() < premium_actual.mean() * 0.9:\n","        print(\"\\n‚ö†Ô∏è  CRITICAL: Specialists under-predicting premium items by >10%\")\n","        print(\"üí° This hurts SMAPE badly! Consider:\")\n","        print(\"   - Increase premium weight in loss (try 5x instead of 3x)\")\n","        print(\"   - Add more premium-specific features\")\n","        print(\"   - Lower premium threshold to get more training data\")\n","    elif premium_preds_spec.mean() > premium_actual.mean() * 1.1:\n","        print(\"\\n‚ö†Ô∏è  Specialists over-predicting premium items by >10%\")\n","    else:\n","        print(\"\\n‚úÖ Specialist predictions are reasonably calibrated\")\n","\n","else:\n","    print(\"\\n‚ö†Ô∏è  No premium samples found in validation set\")\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ VALIDATION COMPLETE\")\n","print(\"=\"*70)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eUsPUxP3wm7X","executionInfo":{"status":"ok","timestamp":1760290970745,"user_tz":-330,"elapsed":81450,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"197ceb7d-6074-4aa0-a08f-866cc1ff5e34"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Using device: cuda\n","\n","======================================================================\n","üìÇ LOADING SPECIALIST MODELS\n","======================================================================\n","‚úÖ Metadata loaded\n","   Input dimension: 1046\n","   Bins: ['affordable', 'mid', 'premium']\n","‚úÖ Loaded 10 models for affordable\n","‚úÖ Loaded 7 models for mid\n","‚úÖ Loaded 5 models for premium\n","\n","======================================================================\n","üìÇ LOADING BASELINE ENSEMBLE (for routing)\n","======================================================================\n","‚úÖ Loaded 10 baseline models\n","\n","======================================================================\n","üß™ PHASE 2: VALIDATION - SPECIALISTS vs BASELINE\n","======================================================================\n","\n","üìä Validation Data:\n","  Total samples: 15000\n","  Affordable (<$40): 12743 (85.0%)\n","  Mid-range ($40-$85): 1671 (11.1%)\n","  Premium (>$85): 586 (3.9%)\n","\n","----------------------------------------------------------------------\n","üìä Method 1: BASELINE ENSEMBLE\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Baseline predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:00<00:00, 95.49it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Baseline Overall SMAPE: 53.37%\n","üìä Baseline Overall MAE: $12.14\n","\n","üìä Baseline Performance by Price Range:\n","  Affordable  :  49.46% SMAPE, $   6.53 MAE (12743 samples)\n","  Mid-range   :  70.29% SMAPE, $  27.56 MAE (1671 samples)\n","  Premium     :  90.23% SMAPE, $  90.31 MAE (586 samples)\n","\n","----------------------------------------------------------------------\n","üìä Method 2: SPECIALIST ENSEMBLES with Routing\n","----------------------------------------------------------------------\n"]},{"output_type":"stream","name":"stderr","text":["Specialist predictions: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [01:20<00:00,  2.67s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","üéØ Specialist Overall SMAPE: 54.65%\n","üìä Specialist Overall MAE: $13.05\n","\n","üìä Specialist Performance by Price Range:\n","  Affordable  :  48.21% SMAPE, $   6.32 MAE (12743 samples)\n","  Mid-range   :  84.80% SMAPE, $  32.08 MAE (1671 samples)\n","  Premium     : 108.68% SMAPE, $ 105.15 MAE (586 samples)\n","\n","======================================================================\n","üèÜ VALIDATION RESULTS COMPARISON\n","======================================================================\n","\n","Method                                SMAPE          MAE\n","----------------------------------------------------------------------\n","Baseline Ensemble                    53.37% $     12.14\n","Specialist Ensembles                 54.65% $     13.05\n","----------------------------------------------------------------------\n","SMAPE Improvement                    -1.28% (-2.39%)\n","\n","‚ö†Ô∏è  Specialists don't improve SMAPE over baseline\n","üí° Consider: Use baseline for test, or retrain with more premium focus\n","\n","üéØ Routing Accuracy: 87.4%\n","   (How often baseline correctly predicts price range)\n","\n","======================================================================\n","üìà PREMIUM PREDICTION ANALYSIS (Critical for SMAPE)\n","======================================================================\n","\n","üìä Premium Item Analysis (586 samples, >$85):\n","  Actual mean:              $142.96\n","  Specialist predictions:   $48.95\n","  Baseline predictions:     $54.60\n","\n","  Specialist Premium SMAPE: 108.68%\n","  Baseline Premium SMAPE:   90.23%\n","  Specialist Premium MAE:   $105.15\n","  Baseline Premium MAE:     $90.31\n","\n","‚ö†Ô∏è  CRITICAL: Specialists under-predicting premium items by >10%\n","üí° This hurts SMAPE badly! Consider:\n","   - Increase premium weight in loss (try 5x instead of 3x)\n","   - Add more premium-specific features\n","   - Lower premium threshold to get more training data\n","\n","======================================================================\n","‚úÖ VALIDATION COMPLETE\n","======================================================================\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["\n","# =========================================================================\n","# PHASE 3: TEST SET PREDICTIONS\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üöÄ PHASE 3: TEST SET PREDICTIONS\")\n","print(\"=\"*70)\n","\n","# Load test data\n","test_data_path = '/content/drive/MyDrive/amazon_ml_challenge/combined_CLIP_final/clip_test_with_alignment.pt'\n","test_data = torch.load(test_data_path)\n","\n","test_input = test_data['test_input'].to(device)\n","test_ids = test_data['test_ids']\n","\n","print(f\"‚úÖ Test data loaded: {test_input.shape}\")\n","print(f\"   Number of test samples: {len(test_ids)}\")\n","\n","# Choose which method to use\n","if use_specialists:\n","    print(f\"\\n‚ú® Using SPECIALIST ensembles for predictions\")\n","    prediction_method = \"specialists\"\n","else:\n","    print(f\"\\n‚ú® Using BASELINE ensemble for predictions\")\n","    prediction_method = \"baseline\"\n","\n","# Make predictions\n","test_loader = DataLoader(\n","    TensorDataset(test_input),\n","    batch_size=batch_size,\n","    shuffle=False\n",")\n","\n","test_preds = []\n","\n","if prediction_method == \"specialists\":\n","    for (batch_x,) in tqdm(test_loader, desc=\"Test predictions (specialists)\"):\n","        pred = predict_with_routing(batch_x.to(device), baseline_models, specialists)\n","        test_preds.append(pred.cpu())\n","else:\n","    for (batch_x,) in tqdm(test_loader, desc=\"Test predictions (baseline)\"):\n","        pred = ensemble_predict(baseline_models, batch_x.to(device))\n","        test_preds.append(pred.cpu())\n","\n","test_preds = torch.cat(test_preds)\n","test_preds_orig = np.expm1(test_preds.numpy().squeeze())\n","\n","# Post-processing\n","print(f\"\\nüîß Post-processing predictions...\")\n","print(f\"   Before clipping - Min: ${test_preds_orig.min():.2f}, Max: ${test_preds_orig.max():.2f}\")\n","\n","# Clip extreme predictions\n","test_preds_orig = np.clip(test_preds_orig, 0, 4000)\n","\n","print(f\"   After clipping - Min: ${test_preds_orig.min():.2f}, Max: ${test_preds_orig.max():.2f}\")\n","print(f\"   Mean: ${test_preds_orig.mean():.2f}, Median: ${np.median(test_preds_orig):.2f}\")\n","\n","# =========================================================================\n","# CREATE SUBMISSION FILE\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üíæ CREATING SUBMISSION FILE\")\n","print(\"=\"*70)\n","\n","submission_df = pd.DataFrame({\n","    'sample_id': test_ids,\n","    'price': test_preds_orig\n","})\n","\n","# Save submission\n","submission_df.to_csv(\"submission_specialists.csv\", index=False)\n","\n","\n","from google.colab import files\n","# Download the CSV file\n","files.download(\"submission_using_specialists.csv\")\n","\n","print(f\"\\nüìä Submission Statistics:\")\n","print(f\"   Total predictions: {len(submission_df)}\")\n","print(f\"   Price range: ${test_preds_orig.min():.2f} - ${test_preds_orig.max():.2f}\")\n","print(f\"   Mean price: ${test_preds_orig.mean():.2f}\")\n","print(f\"   Median price: ${np.median(test_preds_orig):.2f}\")\n","\n","# Distribution analysis\n","bins_counts = {\n","    'Affordable (<$50)': (test_preds_orig < 50).sum(),\n","    'Mid-range ($50-$100)': ((test_preds_orig >= 50) & (test_preds_orig < 100)).sum(),\n","    'Premium ($100+)': (test_preds_orig >= 100).sum()\n","}\n","\n","print(f\"\\nüìà Test Predictions Distribution:\")\n","for bin_name, count in bins_counts.items():\n","    pct = (count / len(test_preds_orig)) * 100\n","    print(f\"   {bin_name:25} {count:6d} samples ({pct:5.1f}%)\")\n","\n","print(f\"\\nüéØ First 10 predictions:\")\n","print(submission_df.head(10))\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ ALL PHASES COMPLETE!\")\n","print(\"=\"*70)\n","print(f\"\\nüìù Summary:\")\n","print(f\"   Baseline MAE: ${baseline_mae:.2f}\")\n","print(f\"   Specialist MAE: ${specialist_mae:.2f}\")\n","# print(f\"   Improvement: ${improvement:.2f} ({improvement_pct:+.2f}%)\")\n","# print(f\"   Submission file: {submission_path}\")\n","print(f\"\\nüöÄ Ready to submit!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":662},"id":"fBO9-t5SxA8Z","executionInfo":{"status":"error","timestamp":1760290975870,"user_tz":-330,"elapsed":5116,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"f7eaa572-e069-4a96-f1fa-18b2b939595f"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üöÄ PHASE 3: TEST SET PREDICTIONS\n","======================================================================\n","‚úÖ Test data loaded: torch.Size([75000, 1046])\n","   Number of test samples: 75000\n","\n","‚ú® Using BASELINE ensemble for predictions\n"]},{"output_type":"stream","name":"stderr","text":["Test predictions (baseline): 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 147/147 [00:01<00:00, 122.31it/s]\n"]},{"output_type":"stream","name":"stdout","text":["\n","üîß Post-processing predictions...\n","   Before clipping - Min: $3.00, Max: $518.81\n","   After clipping - Min: $3.00, Max: $518.81\n","   Mean: $19.96, Median: $15.76\n","\n","======================================================================\n","üíæ CREATING SUBMISSION FILE\n","======================================================================\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"Cannot find file: submission_using_specialists.csv","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-182690347.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;31m# Download the CSV file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"submission_using_specialists.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nüìä Submission Statistics:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Cannot find file: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=undefined-variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m   \u001b[0mcomm_manager\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_IPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomm_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Cannot find file: submission_using_specialists.csv"]}]},{"cell_type":"code","source":[],"metadata":{"id":"NdEjFA77MfJm"},"execution_count":null,"outputs":[]}]}