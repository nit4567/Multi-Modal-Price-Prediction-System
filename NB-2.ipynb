{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMR+5j2aC3Nlu3be8WAlhn7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Utils"],"metadata":{"id":"IknJdwNr9gKw"}},{"cell_type":"code","source":["#################################################################################\n","import numpy as np\n","import re\n","import pandas as pd\n","\n","# constants\n","OZ_TO_ML = 29.5735\n","OZ_TO_G  = 28.3495\n","L_TO_ML  = 1000.0\n","KG_TO_G  = 1000.0\n","LB_TO_G  = 453.59237\n","\n","\n","UNIT_MAP = {\n","    \"oz\": \"oz\", \"ounce\": \"oz\", \"ounces\": \"oz\",\n","    \"fl oz\": \"fl_oz\", \"floz\": \"fl_oz\", \"fl.oz\": \"fl_oz\",\n","    \"ml\": \"ml\", \"l\": \"l\", \"litre\": \"l\", \"liter\": \"l\",\n","    \"g\": \"g\", \"kg\": \"kg\",\n","    \"pack\": \"pack\", \"count\": \"count\", \"ct\": \"count\", \"piece\": \"count\"\n","}\n","\n","def extract_value_and_unit(text):\n","    \"\"\"\n","    Extract 'value' and 'unit' (explicit or implicit) from product text.\n","    Requires 'Value:' and 'Unit:' to appear on their own lines.\n","    \"\"\"\n","    t = str(text).lower()\n","    out = {\"value\": np.nan, \"unit\": None, \"has_value\": 0, \"has_unit\": 0}\n","\n","    # 1Ô∏è‚É£ Explicit \"Value:\" label at start of line\n","    m = re.search(r'(?m)^[ \\t]*value:\\s*(\\d+(?:\\.\\d+)?)', t)\n","    if m:\n","        out[\"value\"] = float(m.group(1))\n","        out[\"has_value\"] = 1\n","\n","    # 2Ô∏è‚É£ Explicit \"Unit:\" label at start of line\n","    m = re.search(r'(?m)^[ \\t]*unit:\\s*([^\\n\\r]*)', t)\n","    if m:\n","        unit_raw = m.group(1).strip()\n","        unit_clean = UNIT_MAP.get(unit_raw.replace('.', '').replace(' ', ''), unit_raw)\n","        out[\"unit\"] = unit_clean\n","        out[\"has_unit\"] = 1\n","\n","    # 3Ô∏è‚É£ Implicit numeric + unit pattern (fallback)\n","    if not out[\"has_unit\"]:\n","        m = re.search(r'(\\d+(?:\\.\\d+)?)\\s*(fl\\.?\\s?oz|ounce|ounces|oz|ml|g|kg|l)\\b', t)\n","        if m:\n","            val = float(m.group(1))\n","            unit_raw = m.group(2).replace('.', '').replace(' ', '')\n","            out[\"value\"] = val if np.isnan(out[\"value\"]) else out[\"value\"]\n","            out[\"unit\"] = UNIT_MAP.get(unit_raw, unit_raw)\n","            out[\"has_unit\"] = 1\n","            out[\"has_value\"] = 1\n","\n","    return out\n","\n","\n","def add_value_unit_features(df, text_col=\"catalog_content\"):\n","    \"\"\"\n","    Adds 'value_only', 'unit_only', 'has_value', and 'has_unit' columns.\n","    \"\"\"\n","    extracted = df[text_col].fillna(\"\").apply(extract_value_and_unit)\n","    extracted_df = pd.DataFrame(list(extracted))\n","\n","    df[\"value\"] = extracted_df[\"value\"]\n","    df[\"unit\"] = extracted_df[\"unit\"]\n","    df[\"has_value\"] = extracted_df[\"has_value\"]\n","    df[\"has_unit\"] = extracted_df[\"has_unit\"]\n","\n","    return df\n","\n","UNIT_MAP_CLEAN = {\n","    # --- volume ---\n","    \"ml\": \"ml\", \"millilitre\": \"ml\", \"milliliter\": \"ml\", \"mililitro\": \"ml\", \"ltr\": \"l\", \"l\": \"l\", \"liters\": \"l\", \"2.5 gal.\": \"gal\",\n","    \"fl_oz\": \"fl_oz\", \"fl ounce\": \"fl_oz\", \"fl oz\": \"fl_oz\", \"fluid ounce\": \"fl_oz\", \"fluid ounces\": \"fl_oz\", \"fluid ounce(s)\": \"fl_oz\", \"20 oz.\": \"oz\",\n","\n","    # --- weight ---\n","    \"g\": \"g\", \"gram\": \"g\", \"grams\": \"g\", \"gramm\": \"g\", \"gr\": \"g\", \"grams(gm)\": \"g\",\n","    \"kg\": \"kg\", \"pound\": \"lb\", \"pounds\": \"lb\", \"lb\": \"lb\",\n","\n","    # --- count / packaging ---\n","    \"pack\": \"pack\", \"packs\": \"pack\", \"per package\": \"pack\", \"per box\": \"pack\",\n","    \"count\": \"count\", \"ct\": \"count\", \"each\": \"count\", \"each / pack: 1\": \"count\",\n","    \"bag\": \"pack\", \"box\": \"pack\", \"box/12\": \"pack\", \"carton\": \"pack\", \"case\": \"pack\",\n","    \"bottle\": \"count\", \"bottles\": \"count\", \"jar\": \"count\", \"can\": \"count\", \"capsule\": \"count\",\n","    \"pouch\": \"count\", \"bucket\": \"count\", \"k-cups\": \"count\", \"ziplock bags\": \"count\", \"paper cupcake liners\": \"count\", \"tea bags\": \"count\",\n","\n","    # --- others (dimensional / irrelevant) ---\n","    \"in\": \"in\", \"sq ft\": \"sq_ft\", \"foot\": \"ft\", \"cm/inch)\": \"cm_inch\",\n","\n","    # --- noise / invalid ---\n","    \"none\": None, \"\": None, \"-\": None, \"---\": None, \"1\": None, \"24\": None,\n","    \"product_weight\": None, \"units\": None,\n","    \"1 pk. color(s): -black. product type: -permanent. pack quantity: -1. tip type: -chisel. dimensions: overall product weight: -0.06 lbs.\": None,\n","    \"comes as a single 0.1 oz stick for on-the-go use\": None,\n","    \"unit√†\": None,\n","    \"7,2 oz\": \"oz\"\n","}\n","\n","def normalize_unit(unit):\n","    if pd.isna(unit) or not isinstance(unit, str):\n","        return None\n","    u = unit.strip().lower()\n","    u = u.replace('.', '').replace('(', '').replace(')', '').replace(':', '').strip()\n","    return UNIT_MAP_CLEAN.get(u, u)  # fallback: return itself if not found\n","\n","UNIT_FINAL_MAP = {\n","    \"oz\": \"oz\",\n","    \"fl_oz\": \"fl_oz\",\n","    \"count\": \"count\",\n","    \"lb\": \"lb\",\n","    \"g\": \"g\",\n","    \"ml\": \"ml\",\n","    \"l\": \"l\",\n","    \"kg\": \"kg\",\n","    \"pack\": \"pack\",\n","    \"per carton\": \"pack\",\n","    \"sq_ft\": \"sq_ft\",\n","    \"ft\": \"ft\",\n","    \"in\": \"in\",\n","    \"8\": None, # Map '8' to None\n","    \"gramsgm\": \"g\",\n","    None: None\n","}\n","\n","UNIT_CATEGORY_MAP = {\n","    \"oz\": \"weight\",\n","    \"lb\": \"weight\",\n","    \"g\": \"weight\",\n","    \"kg\": \"weight\",\n","    \"ml\": \"volume\",\n","    \"l\": \"volume\",\n","    \"fl_oz\": \"volume\",\n","    \"count\": \"count\",\n","    \"pack\": \"count\",\n","    \"sq_ft\": \"dimension\",\n","    \"ft\": \"dimension\",\n","    \"in\": \"dimension\",\n","    None: \"unknown\"\n","}\n","\n","\n","def clean_catalog_text(text):\n","    \"\"\"\n","    Removes lines starting with 'Value:' or 'Unit:' (case-insensitive) from catalog text.\n","    Only removes if they are at the beginning of a line.\n","    \"\"\"\n","    pattern = r'(?im)^[ \\t]*(value:.*|unit:.*)$'\n","    cleaned = re.sub(pattern, '', str(text))\n","    return cleaned.strip()\n","\n","def qty_to_base(qty, unit_final, unit_category):\n","    \"\"\"Return (qty_in_base, base_type) where base_type is 'ml', 'g', or 'count' or None\"\"\"\n","    if pd.isna(qty) or qty <= 0 or unit_final is None:\n","        return (np.nan, None)\n","    u = str(unit_final).lower()\n","    if unit_category == 'volume':\n","        if u in ('ml',):\n","            return (qty, 'ml')\n","        if u in ('l', 'ltr'):\n","            return (qty * L_TO_ML, 'ml')\n","        if u in ('fl_oz','floz','fl ounce','fluid ounce','fluid ounces'):\n","            return (qty * OZ_TO_ML, 'ml')\n","        if u == 'oz':  # ambiguous: treat by category; here category=volume so ml\n","            return (qty * OZ_TO_ML, 'ml')\n","    if unit_category == 'weight':\n","        if u in ('g','gram','grams','gr','gramsgm'):\n","            return (qty, 'g')\n","        if u in ('kg',):\n","            return (qty * KG_TO_G, 'g')\n","        if u in ('lb','pound','pounds'):\n","            return (qty * LB_TO_G, 'g')\n","        if u == 'oz':  # treat ounce as weight here\n","            return (qty * OZ_TO_G, 'g')\n","    if unit_category == 'count':\n","        return (qty, 'count')\n","    return (np.nan, None)\n","\n","def feat_eng(data):\n","    \"\"\"Applies feature engineering steps to the input DataFrame.\"\"\"\n","\n","    # 1) price_per_unit (raw)\n","\n","    # 2) standardized base price (price per ml or per g)\n","    # Ensure 'unit_final' and 'unit_category' columns exist\n","    if 'unit_final' not in data.columns or 'unit_category' not in data.columns:\n","         raise ValueError(\"DataFrame must contain 'unit_final' and 'unit_category' columns before calling feat_eng.\")\n","\n","    qty_base = data.apply(lambda r: qty_to_base(r['value'], r['unit_final'], r['unit_category']), axis=1)\n","    data['qty_base'] = [q[0] for q in qty_base]\n","    data['base_type'] = [q[1] for q in qty_base]\n","\n","    # 4) simple text features (use catalog_content_clean or catalog_content which you replaced)\n","    def text_feats(t):\n","        t = str(t)\n","        words = re.findall(r'\\w+', t)\n","        word_count = len(words)\n","        char_count = len(t)\n","        avg_word_len = np.mean([len(w) for w in words]) if words else 0\n","        bullet_count = len(re.findall(r'bullet', t, flags=re.I))  # simple bullet marker\n","        digits = len(re.findall(r'\\d', t))\n","        return pd.Series([word_count, char_count, avg_word_len, bullet_count, digits])\n","\n","    # Ensure 'catalog_content' column exists\n","    if 'catalog_content' not in data.columns:\n","         raise ValueError(\"DataFrame must contain 'catalog_content' column before calling feat_eng.\")\n","\n","    data[['word_count','char_count','avg_word_len','bullet_count','num_digits']] = data['catalog_content'].apply(text_feats)\n","\n","    # 5) keyword flags\n","    keywords = {\n","        'organic': 'is_organic',\n","        'gluten-free': 'is_gluten_free',\n","        'gluten free': 'is_gluten_free',\n","        'sugar-free': 'is_sugar_free',\n","        'sugar free': 'is_sugar_free',\n","        'vegan': 'is_vegan',\n","        'new': 'is_new',\n","        'pack': 'has_pack_word',\n","        'bundle': 'has_bundle'\n","    }\n","    for kw, col in keywords.items():\n","        data[col] = data['catalog_content'].str.contains(re.escape(kw), case=False, na=False).astype(int)\n","\n","    return data # Return the modified DataFrame\n","def process_FE(data):\n","  data = add_value_unit_features(data)\n","\n","  data[\"unit_normalized\"] = data[\"unit\"].apply(normalize_unit)\n","  data[\"unit_final\"] = data[\"unit_normalized\"].map(UNIT_FINAL_MAP) # Applied to data DataFrame\n","  data[\"unit_category\"] = data[\"unit_final\"].map(UNIT_CATEGORY_MAP).fillna(\"unknown\") # Applied to data DataFrame\n","  data['catalog_content'] = data['catalog_content'].apply(clean_catalog_text)\n","\n","  # Drop unneeded columns to save space\n","  cols_to_drop = [\"unit\", \"unit_normalized\"]\n","  data = data.drop(columns=cols_to_drop)\n","  data = feat_eng(data)\n","  return data\n"],"metadata":{"id":"dddiV2-8AVIJ","executionInfo":{"status":"ok","timestamp":1760249741773,"user_tz":-330,"elapsed":472,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["## Clip"],"metadata":{"id":"KSzWq2g-9c8Z"}},{"cell_type":"code","source":["!pip install git+https://github.com/openai/CLIP.git ftfy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CRAc8fXAGV4n","executionInfo":{"status":"ok","timestamp":1760250522416,"user_tz":-330,"elapsed":11989,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"ff906171-8bff-4c69-f6ed-0239f76136a9"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-pm2poqgf\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-pm2poqgf\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting ftfy\n","  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (25.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (2.8.0+cu126)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from clip==1.0) (0.23.0+cu126)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from ftfy) (0.2.14)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.20.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (4.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (75.2.0)\n","Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.13.3)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.80)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (9.10.2.21)\n","Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.4.1)\n","Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.3.0.4)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (10.3.7.77)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (11.7.1.2)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.5.4.2)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (0.7.1)\n","Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (2.27.3)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.77)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (12.6.85)\n","Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (1.11.1.6)\n","Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch->clip==1.0) (3.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->clip==1.0) (11.3.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->clip==1.0) (3.0.3)\n","Downloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: clip\n","  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369490 sha256=93e63097752f75c2465afd39c6af40427c110ec452b9cf19d2b5bd0ea7978a06\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-bju1wkmu/wheels/35/3e/df/3d24cbfb3b6a06f17a2bfd7d1138900d4365d9028aa8f6e92f\n","Successfully built clip\n","Installing collected packages: ftfy, clip\n","Successfully installed clip-1.0 ftfy-6.3.1\n"]}]},{"cell_type":"code","source":[" #-------------------------\n","# IMPROVED PIPELINE: Add CLIP Text + Alignment\n","# -------------------------\n","import torch\n","import clip\n","import numpy as np\n","from tqdm import tqdm\n","import os\n","import pandas as pd\n","from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define base directory\n","base_dir = '/content/drive/MyDrive/amazon_ml_challenge'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bxyeKuodEy2T","executionInfo":{"status":"ok","timestamp":1760250561142,"user_tz":-330,"elapsed":38717,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"f9f4a3e5-8e66-42a0-ca7f-4d3a1abfa054"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# #\n","# # -------------------------\n","# # Load and process data\n","# # -------------------------\n","# train_csv_path = os.path.join(base_dir, 'train.csv')\n","# data = pd.read_csv(train_csv_path)\n","# data = process_FE(data)\n","\n","# # Impute missing values\n","# data['value'] = data['value'].fillna(-1)\n","# median_qty_base = data['qty_base'].median()\n","# data['qty_base'] = data['qty_base'].fillna(median_qty_base)\n","# data['base_type'] = data['base_type'].fillna('missing')\n","# data['unit_final'] = data['unit_final'].fillna('unknown')\n","\n","# print(\"‚úÖ Data loaded and processed\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"29HucdL9C-0u","executionInfo":{"status":"ok","timestamp":1760250025139,"user_tz":-330,"elapsed":33489,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"70926263-3cfe-4a15-a060-57ff271b7ece"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Data loaded and processed\n"]}]},{"cell_type":"code","source":["# save_path_drive = os.path.join(base_dir, \"processed_fe+textpre.csv\")\n","# data.to_csv(save_path_drive, index=False)\n","# print(f\"‚úÖ Processed data saved to: {save_path_drive}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0vmIGhXdE--4","executionInfo":{"status":"ok","timestamp":1760250143183,"user_tz":-330,"elapsed":3752,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"d913e200-9695-45d4-a07c-9a6130bda41f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Processed data saved to: /content/drive/MyDrive/amazon_ml_challenge/processed_fe+textpre.csv\n"]}]},{"cell_type":"code","source":["# Load processed data directly from Drive\n","processed_csv_path = os.path.join(base_dir, \"processed_fe+textpre.csv\")\n","data = pd.read_csv(processed_csv_path)\n","print(f\"‚úÖ Processed data loaded from: {processed_csv_path}\")\n","display(data.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":533},"id":"Bn-y2JZJFsxM","executionInfo":{"status":"ok","timestamp":1760250565591,"user_tz":-330,"elapsed":4447,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"e0e93eb3-6e21-46a4-9ca9-97508b19a39d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Processed data loaded from: /content/drive/MyDrive/amazon_ml_challenge/processed_fe+textpre.csv\n"]},{"output_type":"display_data","data":{"text/plain":["   sample_id                                    catalog_content  \\\n","0      33127  Item Name: La Victoria Green Taco Sauce Mild, ...   \n","1     198967  Item Name: Salerno Cookies, The Original Butte...   \n","2     261251  Item Name: Bear Creek Hearty Soup Bowl, Creamy...   \n","3      55858  Item Name: Judee‚Äôs Blue Cheese Powder 11.25 oz...   \n","4     292686  Item Name: kedem Sherry Cooking Wine, 12.7 Oun...   \n","\n","                                          image_link  price  value  has_value  \\\n","0  https://m.media-amazon.com/images/I/51mo8htwTH...   4.89  72.00          1   \n","1  https://m.media-amazon.com/images/I/71YtriIHAA...  13.12  32.00          1   \n","2  https://m.media-amazon.com/images/I/51+PFEe-w-...   1.97  11.40          1   \n","3  https://m.media-amazon.com/images/I/41mu0HAToD...  30.34  11.25          1   \n","4  https://m.media-amazon.com/images/I/41sA037+Qv...  66.49  12.00          1   \n","\n","   has_unit unit_final unit_category     qty_base  ... avg_word_len  \\\n","0         1      fl_oz        volume  2129.292000  ...     3.846154   \n","1         1         oz        weight   907.184000  ...     5.184211   \n","2         1         oz        weight   323.184300  ...     4.250000   \n","3         1         oz        weight   318.931875  ...     5.009615   \n","4         1      count         count    12.000000  ...     4.041667   \n","\n","   bullet_count  num_digits  is_organic  is_gluten_free  is_sugar_free  \\\n","0           0.0         3.0           0               0              0   \n","1           5.0        14.0           0               0              0   \n","2           5.0         9.0           0               0              0   \n","3           5.0        13.0           0               0              0   \n","4           1.0        10.0           0               0              0   \n","\n","   is_vegan  is_new  has_pack_word  has_bundle  \n","0         0       0              1           0  \n","1         0       0              1           0  \n","2         0       0              1           0  \n","3         0       0              0           0  \n","4         0       0              0           0  \n","\n","[5 rows x 23 columns]"],"text/html":["\n","  <div id=\"df-ae1bd9b0-028a-49d2-9d45-94f1b30a3f15\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sample_id</th>\n","      <th>catalog_content</th>\n","      <th>image_link</th>\n","      <th>price</th>\n","      <th>value</th>\n","      <th>has_value</th>\n","      <th>has_unit</th>\n","      <th>unit_final</th>\n","      <th>unit_category</th>\n","      <th>qty_base</th>\n","      <th>...</th>\n","      <th>avg_word_len</th>\n","      <th>bullet_count</th>\n","      <th>num_digits</th>\n","      <th>is_organic</th>\n","      <th>is_gluten_free</th>\n","      <th>is_sugar_free</th>\n","      <th>is_vegan</th>\n","      <th>is_new</th>\n","      <th>has_pack_word</th>\n","      <th>has_bundle</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>33127</td>\n","      <td>Item Name: La Victoria Green Taco Sauce Mild, ...</td>\n","      <td>https://m.media-amazon.com/images/I/51mo8htwTH...</td>\n","      <td>4.89</td>\n","      <td>72.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>fl_oz</td>\n","      <td>volume</td>\n","      <td>2129.292000</td>\n","      <td>...</td>\n","      <td>3.846154</td>\n","      <td>0.0</td>\n","      <td>3.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>198967</td>\n","      <td>Item Name: Salerno Cookies, The Original Butte...</td>\n","      <td>https://m.media-amazon.com/images/I/71YtriIHAA...</td>\n","      <td>13.12</td>\n","      <td>32.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>oz</td>\n","      <td>weight</td>\n","      <td>907.184000</td>\n","      <td>...</td>\n","      <td>5.184211</td>\n","      <td>5.0</td>\n","      <td>14.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>261251</td>\n","      <td>Item Name: Bear Creek Hearty Soup Bowl, Creamy...</td>\n","      <td>https://m.media-amazon.com/images/I/51+PFEe-w-...</td>\n","      <td>1.97</td>\n","      <td>11.40</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>oz</td>\n","      <td>weight</td>\n","      <td>323.184300</td>\n","      <td>...</td>\n","      <td>4.250000</td>\n","      <td>5.0</td>\n","      <td>9.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>55858</td>\n","      <td>Item Name: Judee‚Äôs Blue Cheese Powder 11.25 oz...</td>\n","      <td>https://m.media-amazon.com/images/I/41mu0HAToD...</td>\n","      <td>30.34</td>\n","      <td>11.25</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>oz</td>\n","      <td>weight</td>\n","      <td>318.931875</td>\n","      <td>...</td>\n","      <td>5.009615</td>\n","      <td>5.0</td>\n","      <td>13.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>292686</td>\n","      <td>Item Name: kedem Sherry Cooking Wine, 12.7 Oun...</td>\n","      <td>https://m.media-amazon.com/images/I/41sA037+Qv...</td>\n","      <td>66.49</td>\n","      <td>12.00</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>count</td>\n","      <td>count</td>\n","      <td>12.000000</td>\n","      <td>...</td>\n","      <td>4.041667</td>\n","      <td>1.0</td>\n","      <td>10.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows √ó 23 columns</p>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ae1bd9b0-028a-49d2-9d45-94f1b30a3f15')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-ae1bd9b0-028a-49d2-9d45-94f1b30a3f15 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-ae1bd9b0-028a-49d2-9d45-94f1b30a3f15');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","    <div id=\"df-477fd477-c199-437e-ab81-a525522007c4\">\n","      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-477fd477-c199-437e-ab81-a525522007c4')\"\n","                title=\"Suggest charts\"\n","                style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","      </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","      <script>\n","        async function quickchart(key) {\n","          const quickchartButtonEl =\n","            document.querySelector('#' + key + ' button');\n","          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","          quickchartButtonEl.classList.add('colab-df-spinner');\n","          try {\n","            const charts = await google.colab.kernel.invokeFunction(\n","                'suggestCharts', [key], {});\n","          } catch (error) {\n","            console.error('Error during call to suggestCharts:', error);\n","          }\n","          quickchartButtonEl.classList.remove('colab-df-spinner');\n","          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","        }\n","        (() => {\n","          let quickchartButtonEl =\n","            document.querySelector('#df-477fd477-c199-437e-ab81-a525522007c4 button');\n","          quickchartButtonEl.style.display =\n","            google.colab.kernel.accessAllowed ? 'block' : 'none';\n","        })();\n","      </script>\n","    </div>\n","\n","    </div>\n","  </div>\n"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"dataframe"}},"metadata":{}}]},{"cell_type":"code","source":["# -------------------------\n","# 1Ô∏è‚É£ Load image embeddings (already aligned to train.csv!)\n","# -------------------------\n","img_data = os.path.join(base_dir, \"full_image_embeddings.npy\")\n","img_embeddings = np.load(img_data)\n","img_tensor = torch.tensor(np.nan_to_num(img_embeddings, nan=0.0), dtype=torch.float)\n","print(f\"‚úÖ Image embeddings loaded: {img_tensor.shape}\")\n","\n","# -------------------------\n","# 2Ô∏è‚É£ Load text+structured embeddings\n","# -------------------------\n","train_input_final_path = os.path.join(base_dir, 'train_input_final.pt')\n","train_text_embd_data = torch.load(train_input_final_path)\n","\n","train_text_struct = train_text_embd_data['train_input']  # MiniLM + structured\n","sample_ids = train_text_embd_data['sample_ids']\n","targets = train_text_embd_data['targets']\n","\n","print(f\"‚úÖ Text+Structured loaded: {train_text_struct.shape}\")\n","\n","# -------------------------\n","# 2.5Ô∏è‚É£ VERIFY ORDER (important sanity check!)\n","# -------------------------\n","print(\"\\nüîç Verifying alignment...\")\n","print(f\"Train CSV has {len(data)} rows\")\n","print(f\"Image embeddings has {len(img_tensor)} rows\")\n","print(f\"Sample IDs has {len(sample_ids)} entries\")\n","\n","# Check if sample_ids match train.csv order\n","if (data['sample_id'].values[:len(sample_ids)] == np.array(sample_ids)).all():\n","    print(\"‚úÖ Sample IDs are in train.csv order - perfect!\")\n","    data_final = data.copy()\n","else:\n","    print(\"‚ö†Ô∏è  Sample IDs are NOT in train.csv order - reordering...\")\n","    data_final = data.set_index('sample_id').loc[sample_ids].reset_index()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"00RBnrh6Ad5W","executionInfo":{"status":"ok","timestamp":1760250588603,"user_tz":-330,"elapsed":22995,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"43b62a1d-dae6-4a5f-9644-a9eb5c9d0d57"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Image embeddings loaded: torch.Size([75000, 512])\n","‚úÖ Text+Structured loaded: torch.Size([75000, 403])\n","\n","üîç Verifying alignment...\n","Train CSV has 75000 rows\n","Image embeddings has 75000 rows\n","Sample IDs has 75000 entries\n","‚úÖ Sample IDs are in train.csv order - perfect!\n"]}]},{"cell_type":"code","source":["\n","# -------------------------\n","# 3Ô∏è‚É£ Load CLIP model for text embeddings\n","# -------------------------\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","clip_model, _ = clip.load(\"ViT-B/32\", device=device)\n","clip_model.eval()\n","print(f\"‚úÖ CLIP model loaded on {device}\")\n","\n","# -------------------------\n","# 4Ô∏è‚É£ Extract CLIP text embeddings\n","# -------------------------\n","def compute_clip_text_embeddings(texts, clip_model, batch_size=256, device='cuda'):\n","    \"\"\"Extract CLIP text embeddings\"\"\"\n","    all_embeds = []\n","\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"CLIP Text Embeddings\"):\n","        batch = texts[i:i+batch_size]\n","        batch_truncated = [str(t)[:300] for t in batch]\n","        text_tokens = clip.tokenize(batch_truncated, truncate=True).to(device)\n","\n","        with torch.no_grad():\n","            features = clip_model.encode_text(text_tokens)\n","            features = features / features.norm(dim=-1, keepdim=True)\n","\n","        all_embeds.append(features.cpu())\n","\n","    return torch.cat(all_embeds, dim=0)\n","\n","train_texts = data_final['catalog_content'].fillna(\"\").tolist()\n","\n","print(\"\\nüöÄ Extracting CLIP text embeddings...\")\n","clip_text_embeddings = compute_clip_text_embeddings(\n","    train_texts,\n","    clip_model,\n","    batch_size=256,\n","    device=device\n",")\n","print(f\"‚úÖ CLIP text embeddings: {clip_text_embeddings.shape}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mDcxneMyCg4d","executionInfo":{"status":"ok","timestamp":1760250664557,"user_tz":-330,"elapsed":75933,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"e4c787d6-af06-4f3d-b873-2f9929eb77af"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 338M/338M [00:01<00:00, 215MiB/s]\n"]},{"output_type":"stream","name":"stdout","text":["‚úÖ CLIP model loaded on cuda\n","\n","üöÄ Extracting CLIP text embeddings...\n"]},{"output_type":"stream","name":"stderr","text":["CLIP Text Embeddings: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 293/293 [01:09<00:00,  4.22it/s]"]},{"output_type":"stream","name":"stdout","text":["‚úÖ CLIP text embeddings: torch.Size([75000, 512])\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["\n","# -------------------------\n","# 5Ô∏è‚É£ Compute Image-Text Alignment (THE SECRET WEAPON!)\n","# -------------------------\n","def compute_alignment_features(image_embeds, text_embeds):\n","    \"\"\"Image-text alignment catches listing quality issues\"\"\"\n","    min_len = min(image_embeds.shape[0], text_embeds.shape[0])\n","    image_embeds = image_embeds[:min_len]\n","    text_embeds = text_embeds[:min_len]\n","\n","    # Cosine similarity (already L2 normalized)\n","    similarity = (image_embeds * text_embeds).sum(dim=1, keepdim=True)\n","\n","    # Binary flags\n","    high_match = (similarity > 0.8).float()\n","    low_match = (similarity < 0.5).float()\n","\n","    return torch.cat([similarity, high_match, low_match], dim=1)\n","\n","print(\"\\nüîó Computing image-text alignment...\")\n","alignment_features = compute_alignment_features(img_tensor, clip_text_embeddings)\n","print(f\"‚úÖ Alignment features: {alignment_features.shape}\")\n","\n","# Statistics\n","print(f\"\\nüìä Alignment Statistics:\")\n","print(f\"   Mean similarity: {alignment_features[:, 0].mean():.3f}\")\n","print(f\"   High matches (>0.8): {alignment_features[:, 1].sum().item():.0f} ({alignment_features[:, 1].mean()*100:.1f}%)\")\n","print(f\"   Low matches (<0.5): {alignment_features[:, 2].sum().item():.0f} ({alignment_features[:, 2].mean()*100:.1f}%)\")\n","\n","# -------------------------\n","# 6Ô∏è‚É£ Extract structured features (last columns from MiniLM+structured)\n","# -------------------------\n","structured_cols = [\n","    'value', 'has_value', 'has_unit',\n","    'unit_final', 'unit_category',\n","    'qty_base', 'base_type',\n","    'word_count', 'char_count', 'avg_word_len',\n","    'bullet_count', 'num_digits',\n","    'is_organic', 'is_gluten_free', 'is_sugar_free',\n","    'is_vegan', 'is_new', 'has_pack_word', 'has_bundle'\n","]\n","num_structured = len(structured_cols)  # ~19 after encoding\n","\n","structured_features = train_text_struct[:, -num_structured:]\n","print(f\"\\nüì¶ Extracted structured features: {structured_features.shape}\")\n","\n","# -------------------------\n","# 7Ô∏è‚É£ Combine everything\n","# -------------------------\n","min_samples = min(\n","    img_tensor.shape[0],\n","    clip_text_embeddings.shape[0],\n","    alignment_features.shape[0],\n","    structured_features.shape[0]\n",")\n","\n","full_input = torch.cat([\n","    img_tensor[:min_samples],              # 512-dim\n","    clip_text_embeddings[:min_samples],    # 512-dim\n","    alignment_features[:min_samples],      # 3-dim\n","    structured_features[:min_samples]      # ~19-dim\n","], dim=1)\n","\n","targets = targets[:min_samples]\n","\n","print(f\"\\n‚úÖ FINAL Combined input: {full_input.shape}\")\n","print(f\"   - CLIP Image: 512 dims\")\n","print(f\"   - CLIP Text: 512 dims\")\n","print(f\"   - Alignment: 3 dims\")\n","print(f\"   - Structured: {structured_features.shape[1]} dims\")\n","print(f\"   - TOTAL: {full_input.shape[1]} dims\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBN1H4tyDghP","executionInfo":{"status":"ok","timestamp":1760250691216,"user_tz":-330,"elapsed":419,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"69201abb-71a9-4249-bd97-f3efc1a23af4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîó Computing image-text alignment...\n","‚úÖ Alignment features: torch.Size([75000, 3])\n","\n","üìä Alignment Statistics:\n","   Mean similarity: 0.207\n","   High matches (>0.8): 0 (0.0%)\n","   Low matches (<0.5): 75000 (100.0%)\n","\n","üì¶ Extracted structured features: torch.Size([75000, 19])\n","\n","‚úÖ FINAL Combined input: torch.Size([75000, 1046])\n","   - CLIP Image: 512 dims\n","   - CLIP Text: 512 dims\n","   - Alignment: 3 dims\n","   - Structured: 19 dims\n","   - TOTAL: 1046 dims\n"]}]},{"cell_type":"code","source":["\n","# -------------------------\n","# 8Ô∏è‚É£ Train/Val Split\n","# -------------------------\n","from sklearn.model_selection import train_test_split\n","\n","train_input_np, val_input_np, train_targets_np, val_targets_np = train_test_split(\n","    full_input.numpy(), targets.numpy(), test_size=0.2, random_state=42\n",")\n","\n","train_input = torch.tensor(train_input_np, dtype=torch.float)\n","val_input = torch.tensor(val_input_np, dtype=torch.float)\n","train_targets = torch.tensor(train_targets_np, dtype=torch.float)\n","val_targets = torch.tensor(val_targets_np, dtype=torch.float)\n","\n","print(f\"\\n‚úÖ Train: {train_input.shape}, Val: {val_input.shape}\")\n","\n","# -------------------------\n","# 9Ô∏è‚É£ Save\n","# -------------------------\n","save_dir = os.path.join(base_dir, \"combined_CLIP_final\")\n","os.makedirs(save_dir, exist_ok=True)\n","save_path = os.path.join(save_dir, \"clip_full_with_alignment.pt\")\n","\n","torch.save({\n","    \"train_input\": train_input,\n","    \"val_input\": val_input,\n","    \"train_targets\": train_targets,\n","    \"val_targets\": val_targets\n","}, save_path)\n","\n","print(f\"\\n‚úÖ Saved to: {save_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yfc40E8ODzLh","executionInfo":{"status":"ok","timestamp":1760250703737,"user_tz":-330,"elapsed":3224,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"88cf47fd-6618-4c1c-ab82-ee9e30a84d0f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","‚úÖ Train: torch.Size([60000, 1046]), Val: torch.Size([15000, 1046])\n","\n","‚úÖ Saved to: /content/drive/MyDrive/amazon_ml_challenge/combined_CLIP_final/clip_full_with_alignment.pt\n"]}]},{"cell_type":"code","source":["# =========================================================================\n","# Load the CLIP-enhanced data\n","# =========================================================================\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","from tqdm import tqdm\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Load your saved CLIP data\n","# save_path = '/content/drive/MyDrive/amazon_ml_challenge/combined_CLIP_final/clip_full_with_alignment.pt'\n","# data = torch.load(save_path)\n","\n","# train_input = data['train_input'].to(device)\n","# val_input = data['val_input'].to(device)\n","# train_targets = data['train_targets'].to(device)\n","# val_targets = data['val_targets'].to(device)\n","\n","print(f\"‚úÖ Data loaded\")\n","print(f\"Train: {train_input.shape}, Val: {val_input.shape}\")\n","print(f\"Input dimensions: {train_input.shape[1]}\")\n","\n","# =========================================================================\n","# Model Definition (Updated for 1046 dims)\n","# =========================================================================\n","class RegressionMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# =========================================================================\n","# Ensemble Prediction Function\n","# =========================================================================\n","def ensemble_predict(models, x):\n","    \"\"\"Average predictions from multiple models\"\"\"\n","    predictions = []\n","    for model in models:\n","        model.eval()\n","        with torch.no_grad():\n","            pred = model(x)\n","            predictions.append(pred)\n","    return torch.stack(predictions).mean(dim=0)\n","\n","# =========================================================================\n","# Training: Large Ensemble (10 models)\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üöÄ Training Large Ensemble (10 models with CLIP features)\")\n","print(\"=\"*70)\n","\n","models_large = []\n","results = {}\n","\n","# For comparison with original\n","val_true_orig = np.expm1(val_targets.cpu().numpy().squeeze())\n","\n","for seed in range(10):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","\n","    print(f\"\\nüîÑ Training model {seed+1}/10 (seed={seed})...\")\n","\n","    model = RegressionMLP(train_input.shape[1]).to(device)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n","    criterion = nn.SmoothL1Loss()\n","\n","    train_loader = DataLoader(\n","        TensorDataset(train_input, train_targets),\n","        batch_size=256,\n","        shuffle=True\n","    )\n","\n","    # Training loop with progress bar\n","    for epoch in range(15):\n","        model.train()\n","        epoch_loss = 0\n","\n","        for xb, yb in train_loader:\n","            xb, yb = xb.to(device), yb.to(device)\n","            optimizer.zero_grad()\n","            preds = model(xb)\n","            loss = criterion(preds, yb)\n","            loss.backward()\n","            optimizer.step()\n","            epoch_loss += loss.item()\n","\n","        # Print progress every 5 epochs\n","        if (epoch + 1) % 5 == 0:\n","            avg_loss = epoch_loss / len(train_loader)\n","            print(f\"   Epoch {epoch+1}/15: Loss = {avg_loss:.4f}\")\n","\n","    models_large.append(model)\n","    print(f\"‚úÖ Model {seed+1}/10 complete\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"GeNTV7aTHiGW","executionInfo":{"status":"error","timestamp":1760251358178,"user_tz":-330,"elapsed":180898,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"b9e9bcb0-5b79-4c3c-fad7-b906a7865d51"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Data loaded\n","Train: torch.Size([60000, 1046]), Val: torch.Size([15000, 1046])\n","Input dimensions: 1046\n","\n","======================================================================\n","üöÄ Training Large Ensemble (10 models with CLIP features)\n","======================================================================\n","\n","üîÑ Training model 1/10 (seed=0)...\n","   Epoch 5/15: Loss = 0.2696\n","   Epoch 10/15: Loss = 0.2202\n","   Epoch 15/15: Loss = 0.1853\n","‚úÖ Model 1/10 complete\n","\n","üîÑ Training model 2/10 (seed=1)...\n","   Epoch 5/15: Loss = 0.2694\n","   Epoch 10/15: Loss = 0.2211\n","   Epoch 15/15: Loss = 0.1848\n","‚úÖ Model 2/10 complete\n","\n","üîÑ Training model 3/10 (seed=2)...\n","   Epoch 5/15: Loss = 0.2695\n","   Epoch 10/15: Loss = 0.2197\n","   Epoch 15/15: Loss = 0.1875\n","‚úÖ Model 3/10 complete\n","\n","üîÑ Training model 4/10 (seed=3)...\n","   Epoch 5/15: Loss = 0.2655\n","   Epoch 10/15: Loss = 0.2186\n","   Epoch 15/15: Loss = 0.1822\n","‚úÖ Model 4/10 complete\n","\n","üîÑ Training model 5/10 (seed=4)...\n","   Epoch 5/15: Loss = 0.2698\n","   Epoch 10/15: Loss = 0.2188\n","   Epoch 15/15: Loss = 0.1828\n","‚úÖ Model 5/10 complete\n","\n","üîÑ Training model 6/10 (seed=5)...\n","   Epoch 5/15: Loss = 0.2691\n","   Epoch 10/15: Loss = 0.2214\n","   Epoch 15/15: Loss = 0.1860\n","‚úÖ Model 6/10 complete\n","\n","üîÑ Training model 7/10 (seed=6)...\n","   Epoch 5/15: Loss = 0.2729\n","   Epoch 10/15: Loss = 0.2224\n","   Epoch 15/15: Loss = 0.1875\n","‚úÖ Model 7/10 complete\n","\n","üîÑ Training model 8/10 (seed=7)...\n","   Epoch 5/15: Loss = 0.2653\n","   Epoch 10/15: Loss = 0.2197\n","   Epoch 15/15: Loss = 0.1844\n","‚úÖ Model 8/10 complete\n","\n","üîÑ Training model 9/10 (seed=8)...\n","   Epoch 5/15: Loss = 0.2690\n","   Epoch 10/15: Loss = 0.2198\n","   Epoch 15/15: Loss = 0.1851\n","‚úÖ Model 9/10 complete\n","\n","üîÑ Training model 10/10 (seed=9)...\n","   Epoch 5/15: Loss = 0.2681\n","   Epoch 10/15: Loss = 0.2196\n","   Epoch 15/15: Loss = 0.1822\n","‚úÖ Model 10/10 complete\n","\n","======================================================================\n","üéØ Evaluating Ensemble Performance\n","======================================================================\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"Expected all tensors to be on the same device, but got mat1 is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_addmm)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-2844638761.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Ensemble predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_large\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0mval_preds_orig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_preds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2844638761.py\u001b[0m in \u001b[0;36mensemble_predict\u001b[0;34m(models, x)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2844638761.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# =========================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but got mat1 is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA_addmm)"]}]},{"cell_type":"markdown","source":["take 2"],"metadata":{"id":"HsczL-_qPLeP"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.utils.data import DataLoader, TensorDataset\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","EPOCHS = 30\n","BATCH = 256\n","MAX_GRAD_NORM = 1.0\n","max_price = 4000\n","max_log_value = np.log1p(max_price)\n","\n","models_large = []\n","results = {}\n","\n","# data loaders (use pin_memory and workers for GPU)\n","train_loader = DataLoader(\n","    TensorDataset(train_input, train_targets),\n","    batch_size=BATCH,\n","    shuffle=True,\n","    pin_memory=False,\n","    num_workers=0\n",")\n","val_loader = DataLoader(\n","    TensorDataset(val_input, val_targets),\n","    batch_size=BATCH,\n","    shuffle=False,\n","    pin_memory=False,\n","    num_workers= 0\n",")\n","\n","for seed in range(10):\n","    torch.manual_seed(seed)\n","    np.random.seed(seed)\n","    if device == 'cuda':\n","        torch.cuda.manual_seed_all(seed)\n","\n","    print(f\"\\nüîÑ Training model {seed+1}/10 (seed={seed})...\")\n","\n","    model = RegressionMLP(train_input.shape[1]).to(device)\n","    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=1e-5)\n","    criterion = nn.SmoothL1Loss()\n","    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n","\n","    best_val_loss = float('inf')\n","    best_state = None\n","\n","    scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n","\n","    for epoch in range(EPOCHS):\n","        # ---------- train ----------\n","        model.train()\n","        train_loss = 0.0\n","        for xb, yb in train_loader:\n","            xb = xb.to(device, non_blocking=True)\n","            yb = yb.to(device, non_blocking=True)\n","\n","            optimizer.zero_grad()\n","            with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n","                preds = model(xb)\n","                loss = criterion(preds, yb)\n","\n","            scaler.scale(loss).backward()\n","            # gradient clipping (unscale first)\n","            scaler.unscale_(optimizer)\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n","\n","            scaler.step(optimizer)\n","            scaler.update()\n","\n","            train_loss += loss.item() * xb.size(0)\n","\n","        train_loss = train_loss / len(train_loader.dataset)\n","\n","        # ---------- validate ----------\n","        model.eval()\n","        val_loss = 0.0\n","        all_preds = []\n","        all_targets = []\n","        with torch.no_grad():\n","            for xb, yb in val_loader:\n","                xb = xb.to(device, non_blocking=True)\n","                yb = yb.to(device, non_blocking=True)\n","\n","                with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n","                    preds = model(xb)\n","                    loss = criterion(preds, yb)\n","\n","                val_loss += loss.item() * xb.size(0)\n","\n","                # keep predictions (for potential debug)\n","                all_preds.append(preds.cpu())\n","                all_targets.append(yb.cpu())\n","\n","        val_loss = val_loss / len(val_loader.dataset)\n","        scheduler.step(val_loss)\n","\n","        # save best model (based on val loss)\n","        if val_loss < best_val_loss:\n","            best_val_loss = val_loss\n","            best_state = model.state_dict()\n","\n","        if (epoch + 1) % 5 == 0 or epoch == 0:\n","            print(f\"   Epoch {epoch+1}/{EPOCHS}: train_loss={train_loss:.4f}  val_loss={val_loss:.4f}\")\n","\n","    # After epochs: load best_state if available\n","    if best_state is not None:\n","        model.load_state_dict(best_state)\n","\n","    models_large.append(model)\n","    print(f\"‚úÖ Model {seed+1}/10 complete (best val_loss={best_val_loss:.4f})\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CQQwVxcaPMyL","executionInfo":{"status":"ok","timestamp":1760253401767,"user_tz":-330,"elapsed":507855,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"b22f4d4d-87a9-488b-a237-df9cdd973ab2"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üîÑ Training model 1/10 (seed=0)...\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-1414455545.py:49: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(device == 'cuda'))\n","/tmp/ipython-input-1414455545.py:60: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n","/tmp/ipython-input-1414455545.py:86: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n","  with torch.cuda.amp.autocast(enabled=(device == 'cuda')):\n"]},{"output_type":"stream","name":"stdout","text":["   Epoch 1/30: train_loss=0.9009  val_loss=0.3071\n","   Epoch 5/30: train_loss=0.2590  val_loss=0.2572\n","   Epoch 10/30: train_loss=0.2095  val_loss=0.2422\n","   Epoch 15/30: train_loss=0.1607  val_loss=0.2503\n","   Epoch 20/30: train_loss=0.1205  val_loss=0.2469\n","   Epoch 25/30: train_loss=0.1040  val_loss=0.2470\n","   Epoch 30/30: train_loss=0.0961  val_loss=0.2451\n","‚úÖ Model 1/10 complete (best val_loss=0.2422)\n","\n","üîÑ Training model 2/10 (seed=1)...\n","   Epoch 1/30: train_loss=0.8922  val_loss=0.3153\n","   Epoch 5/30: train_loss=0.2595  val_loss=0.2508\n","   Epoch 10/30: train_loss=0.2106  val_loss=0.2459\n","   Epoch 15/30: train_loss=0.1676  val_loss=0.2461\n","   Epoch 20/30: train_loss=0.1177  val_loss=0.2425\n","   Epoch 25/30: train_loss=0.0999  val_loss=0.2444\n","   Epoch 30/30: train_loss=0.0903  val_loss=0.2516\n","‚úÖ Model 2/10 complete (best val_loss=0.2413)\n","\n","üîÑ Training model 3/10 (seed=2)...\n","   Epoch 1/30: train_loss=0.9048  val_loss=0.3201\n","   Epoch 5/30: train_loss=0.2606  val_loss=0.2647\n","   Epoch 10/30: train_loss=0.2098  val_loss=0.2477\n","   Epoch 15/30: train_loss=0.1707  val_loss=0.2444\n","   Epoch 20/30: train_loss=0.1277  val_loss=0.2555\n","   Epoch 25/30: train_loss=0.1017  val_loss=0.2524\n","   Epoch 30/30: train_loss=0.0933  val_loss=0.2444\n","‚úÖ Model 3/10 complete (best val_loss=0.2388)\n","\n","üîÑ Training model 4/10 (seed=3)...\n","   Epoch 1/30: train_loss=0.8636  val_loss=0.3155\n","   Epoch 5/30: train_loss=0.2618  val_loss=0.2540\n","   Epoch 10/30: train_loss=0.2081  val_loss=0.2485\n","   Epoch 15/30: train_loss=0.1694  val_loss=0.2602\n","   Epoch 20/30: train_loss=0.1241  val_loss=0.2445\n","   Epoch 25/30: train_loss=0.0995  val_loss=0.2462\n","   Epoch 30/30: train_loss=0.0877  val_loss=0.2459\n","‚úÖ Model 4/10 complete (best val_loss=0.2428)\n","\n","üîÑ Training model 5/10 (seed=4)...\n","   Epoch 1/30: train_loss=0.8829  val_loss=0.3056\n","   Epoch 5/30: train_loss=0.2589  val_loss=0.2712\n","   Epoch 10/30: train_loss=0.2085  val_loss=0.2470\n","   Epoch 15/30: train_loss=0.1523  val_loss=0.2452\n","   Epoch 20/30: train_loss=0.1162  val_loss=0.2452\n","   Epoch 25/30: train_loss=0.1015  val_loss=0.2487\n","   Epoch 30/30: train_loss=0.0929  val_loss=0.2473\n","‚úÖ Model 5/10 complete (best val_loss=0.2420)\n","\n","üîÑ Training model 6/10 (seed=5)...\n","   Epoch 1/30: train_loss=0.8865  val_loss=0.3003\n","   Epoch 5/30: train_loss=0.2627  val_loss=0.2602\n","   Epoch 10/30: train_loss=0.2099  val_loss=0.2487\n","   Epoch 15/30: train_loss=0.1556  val_loss=0.2442\n","   Epoch 20/30: train_loss=0.1227  val_loss=0.2650\n","   Epoch 25/30: train_loss=0.1050  val_loss=0.2478\n","   Epoch 30/30: train_loss=0.0980  val_loss=0.2442\n","‚úÖ Model 6/10 complete (best val_loss=0.2409)\n","\n","üîÑ Training model 7/10 (seed=6)...\n","   Epoch 1/30: train_loss=0.8878  val_loss=0.3848\n","   Epoch 5/30: train_loss=0.2603  val_loss=0.2756\n","   Epoch 10/30: train_loss=0.1951  val_loss=0.2454\n","   Epoch 15/30: train_loss=0.1463  val_loss=0.2411\n","   Epoch 20/30: train_loss=0.1178  val_loss=0.2483\n","   Epoch 25/30: train_loss=0.1043  val_loss=0.2441\n","   Epoch 30/30: train_loss=0.0997  val_loss=0.2454\n","‚úÖ Model 7/10 complete (best val_loss=0.2411)\n","\n","üîÑ Training model 8/10 (seed=7)...\n","   Epoch 1/30: train_loss=0.8886  val_loss=0.3647\n","   Epoch 5/30: train_loss=0.2599  val_loss=0.2542\n","   Epoch 10/30: train_loss=0.1944  val_loss=0.2407\n","   Epoch 15/30: train_loss=0.1470  val_loss=0.2427\n","   Epoch 20/30: train_loss=0.1177  val_loss=0.2423\n","   Epoch 25/30: train_loss=0.1077  val_loss=0.2455\n","   Epoch 30/30: train_loss=0.1012  val_loss=0.2440\n","‚úÖ Model 8/10 complete (best val_loss=0.2407)\n","\n","üîÑ Training model 9/10 (seed=8)...\n","   Epoch 1/30: train_loss=0.8922  val_loss=0.3089\n","   Epoch 5/30: train_loss=0.2604  val_loss=0.2634\n","   Epoch 10/30: train_loss=0.2105  val_loss=0.2512\n","   Epoch 15/30: train_loss=0.1711  val_loss=0.2439\n","   Epoch 20/30: train_loss=0.1305  val_loss=0.2437\n","   Epoch 25/30: train_loss=0.1017  val_loss=0.2457\n","   Epoch 30/30: train_loss=0.0852  val_loss=0.2440\n","‚úÖ Model 9/10 complete (best val_loss=0.2437)\n","\n","üîÑ Training model 10/10 (seed=9)...\n","   Epoch 1/30: train_loss=0.8896  val_loss=0.3417\n","   Epoch 5/30: train_loss=0.2591  val_loss=0.2726\n","   Epoch 10/30: train_loss=0.2095  val_loss=0.2447\n","   Epoch 15/30: train_loss=0.1729  val_loss=0.2464\n","   Epoch 20/30: train_loss=0.1270  val_loss=0.2428\n","   Epoch 25/30: train_loss=0.1000  val_loss=0.2440\n","   Epoch 30/30: train_loss=0.0868  val_loss=0.2484\n","‚úÖ Model 10/10 complete (best val_loss=0.2414)\n"]}]},{"cell_type":"code","source":["\n","print(\"\\n\" + \"=\"*70)\n","print(\"üéØ Evaluating Ensemble Performance\")\n","print(\"=\"*70)\n","val_input = val_input.to(device)\n","val_targets = val_targets.to(device)\n","\n","max_price = 4000\n","max_log_value = np.log1p(max_price)\n","\n","# Ensemble predictions\n","val_preds_raw = ensemble_predict(models_large, val_input)\n","\n","# Clip in log-space\n","val_preds_clipped = torch.clamp(val_preds_raw, min=0, max=max_log_value)\n","val_preds_orig = np.expm1(val_preds_clipped.cpu().numpy().squeeze())\n","\n","# Calculate metrics\n","mae_ensemble = mean_absolute_error(val_true_orig, val_preds_orig)\n","rmse_ensemble = np.sqrt(mean_squared_error(val_true_orig, val_preds_orig))\n","\n","# Individual model predictions for comparison\n","print(\"\\nüìä Individual Model Performance:\")\n","individual_maes = []\n","for i, model in enumerate(models_large):\n","    model.eval()\n","    with torch.no_grad():\n","        preds = model(val_input)\n","        preds_clipped = torch.clamp(preds, min=0, max=max_log_value)\n","        preds_orig = np.expm1(preds_clipped.cpu().numpy().squeeze())\n","        mae = mean_absolute_error(val_true_orig, preds_orig)\n","        individual_maes.append(mae)\n","        if i < 3:  # Print first 3\n","            print(f\"   Model {i+1}: ${mae:.2f}\")\n","\n","print(f\"   ...\")\n","print(f\"   Model 10: ${individual_maes[-1]:.2f}\")\n","print(f\"   Individual MAE range: ${min(individual_maes):.2f} - ${max(individual_maes):.2f}\")\n","print(f\"   Individual MAE mean: ${np.mean(individual_maes):.2f}\")\n","\n","# Final results\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèÜ FINAL RESULTS\")\n","print(\"=\"*70)\n","print(f\"Previous (MiniLM + Image):     MAE = $12.16\")\n","print(f\"New (CLIP + Alignment):        MAE = ${mae_ensemble:.2f}\")\n","print(f\"                               RMSE = ${rmse_ensemble:.2f}\")\n","\n","improvement = ((12.16 - mae_ensemble) / 12.16) * 100\n","print(f\"\\n‚ú® Improvement: {improvement:+.1f}%\")\n","\n","if mae_ensemble < 12.16:\n","    print(\"üéâ SUCCESS! The CLIP features improved performance!\")\n","else:\n","    print(\"‚ö†Ô∏è  Performance didn't improve as expected. Possible reasons:\")\n","    print(\"   - Need more training epochs\")\n","    print(\"   - Try different learning rate\")\n","    print(\"   - Check if alignment features are working\")\n","\n","# =========================================================================\n","# Error Analysis by Price Range\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìä Error Analysis by Price Range\")\n","print(\"=\"*70)\n","\n","price_ranges = [\n","    (0, 10, \"$0-$10\"),\n","    (10, 20, \"$10-$20\"),\n","    (20, 50, \"$20-$50\"),\n","    (50, 100, \"$50-$100\"),\n","    (100, float('inf'), \"$100+\")\n","]\n","\n","for low, high, label in price_ranges:\n","    mask = (val_true_orig >= low) & (val_true_orig < high)\n","    if mask.sum() > 0:\n","        range_mae = np.abs(val_true_orig[mask] - val_preds_orig[mask]).mean()\n","        count = mask.sum()\n","        pct = (count / len(val_true_orig)) * 100\n","        print(f\"{label:12} | Count: {count:5} ({pct:5.1f}%) | MAE: ${range_mae:6.2f}\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I5tCIjNIItr8","executionInfo":{"status":"ok","timestamp":1760253553649,"user_tz":-330,"elapsed":397,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"c55bef1e-31ba-4a9d-dbcb-773e5b0f983c"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üéØ Evaluating Ensemble Performance\n","======================================================================\n","\n","üìä Individual Model Performance:\n","   Model 1: $12.50\n","   Model 2: $13.10\n","   Model 3: $12.32\n","   ...\n","   Model 10: $12.91\n","   Individual MAE range: $12.32 - $13.10\n","   Individual MAE mean: $12.67\n","\n","======================================================================\n","üèÜ FINAL RESULTS\n","======================================================================\n","Previous (MiniLM + Image):     MAE = $12.16\n","New (CLIP + Alignment):        MAE = $12.18\n","                               RMSE = $46.24\n","\n","‚ú® Improvement: -0.2%\n","‚ö†Ô∏è  Performance didn't improve as expected. Possible reasons:\n","   - Need more training epochs\n","   - Try different learning rate\n","   - Check if alignment features are working\n","\n","======================================================================\n","üìä Error Analysis by Price Range\n","======================================================================\n","$0-$10       | Count:  5746 ( 38.3%) | MAE: $  5.24\n","$10-$20      | Count:  3837 ( 25.6%) | MAE: $  5.27\n","$20-$50      | Count:  3790 ( 25.3%) | MAE: $ 12.62\n","$50-$100     | Count:  1235 (  8.2%) | MAE: $ 35.41\n","$100+        | Count:   392 (  2.6%) | MAE: $104.31\n"]}]},{"cell_type":"code","source":["# # =========================================================================\n","# # Save the Ensemble Models\n","# # =========================================================================\n","# print(\"\\n\" + \"=\"*70)\n","# print(\"üíæ Saving Models\")\n","# print(\"=\"*70)\n","\n","# save_dir = '/content/drive/MyDrive/amazon_ml_challenge/ensemble_CLIP_models'\n","# import os\n","# os.makedirs(save_dir, exist_ok=True)\n","\n","# for i, model in enumerate(models_large):\n","#     model_path = os.path.join(save_dir, f'clip_ensemble_model_{i}.pt')\n","#     torch.save(model.state_dict(), model_path)\n","\n","# print(f\"‚úÖ Saved 10 models to: {save_dir}\")\n","\n","# # Save metadata\n","# metadata = {\n","#     'mae': mae_ensemble,\n","#     'rmse': rmse_ensemble,\n","#     'individual_maes': individual_maes,\n","#     'improvement_vs_baseline': improvement,\n","#     'input_dim': train_input.shape[1],\n","#     'features': 'CLIP Image (512) + CLIP Text (512) + Alignment (3) + Structured (19)'\n","# }\n","\n","# metadata_path = os.path.join(save_dir, 'ensemble_metadata.pt')\n","# torch.save(metadata, metadata_path)\n","# print(f\"‚úÖ Saved metadata to: {metadata_path}\")\n","\n","# print(\"\\n\" + \"=\"*70)\n","# print(\"‚úÖ ALL DONE! Ready for test predictions\")\n","# print(\"=\"*70)"],"metadata":{"id":"RTspuwDJOtcm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Stacking/Meta-Learning on Top of Ensemble"],"metadata":{"id":"c-Ux3XT99Fcw"}},{"cell_type":"code","source":["# =========================================================================\n","# Stacking/Meta-Learning on Top of Ensemble\n","# Runs entirely on CPU - no GPU needed!\n","# =========================================================================\n","import torch\n","import torch.nn as nn # Import nn for RegressionMLP definition\n","import numpy as np\n","from sklearn.linear_model import Ridge, Lasso\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.metrics import mean_absolute_error, mean_squared_error\n","import pickle\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# =========================================================================\n","# Model Definition (Same as training) - COPIED FROM ABOVE\n","# =========================================================================\n","class RegressionMLP(nn.Module):\n","    def __init__(self, input_dim):\n","        super().__init__()\n","        self.model = nn.Sequential(\n","            nn.Linear(input_dim, 1024),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(1024),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(1024, 512),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(512),\n","            nn.Dropout(0.3),\n","\n","            nn.Linear(512, 256),\n","            nn.ReLU(),\n","            nn.BatchNorm1d(256),\n","            nn.Dropout(0.2),\n","\n","            nn.Linear(256, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üß† STACKING META-MODEL TRAINING\")\n","print(\"=\"*70)\n","\n","# =========================================================================\n","# STEP 0: Load models from Drive (if not already in memory)\n","# =========================================================================\n","# Uncomment this section if starting fresh session:\n","print(\"\\nüìÇ Loading base models from Drive...\")\n","from google.colab import drive\n","\n","# Mount Google Drive\n","# drive.mount('/content/drive') # Already mounted in previous cells\n","\n","save_dir = '/content/drive/MyDrive/amazon_ml_challenge/ensemble_CLIP_models'\n","# Fix: Add weights_only=False to load metadata\n","metadata = torch.load(f'{save_dir}/ensemble_metadata.pt', weights_only=False)\n","input_dim = metadata['input_dim']\n","\n","models_large = []\n","for i in range(10):\n","    # Fix: Add map_location=device for consistency, though models are loaded to GPU in original notebook\n","    model = RegressionMLP(input_dim).to(device)\n","    model.load_state_dict(torch.load(\n","        f'{save_dir}/clip_ensemble_model_{i}.pt',\n","        map_location=device,\n","        weights_only=False # Also add weights_only=False for model state dicts\n","    ))\n","    model.eval()\n","    models_large.append(model)\n","    print(f\"  ‚úì Loaded model {i+1}/10\")\n","print(\"‚úÖ All models loaded!\")\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rF1dvmkL-2aI","executionInfo":{"status":"ok","timestamp":1760265213607,"user_tz":-330,"elapsed":1094,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"2047813f-2cc9-4a53-9774-04e0621c6950"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üß† STACKING META-MODEL TRAINING\n","======================================================================\n","\n","üìÇ Loading base models from Drive...\n","  ‚úì Loaded model 1/10\n","  ‚úì Loaded model 2/10\n","  ‚úì Loaded model 3/10\n","  ‚úì Loaded model 4/10\n","  ‚úì Loaded model 5/10\n","  ‚úì Loaded model 6/10\n","  ‚úì Loaded model 7/10\n","  ‚úì Loaded model 8/10\n","  ‚úì Loaded model 9/10\n","  ‚úì Loaded model 10/10\n","‚úÖ All models loaded!\n"]}]},{"cell_type":"code","source":["import os\n","base_dir = '/content/drive/MyDrive/amazon_ml_challenge'\n","data_save_path = os.path.join(base_dir, \"combined_CLIP_final\", \"clip_full_with_alignment.pt\")\n","\n","# Load data (train/val split)\n","try:\n","    data_loaded = torch.load(data_save_path, map_location=device, weights_only=False)\n","    # train_input = data_loaded['train_input'].to(device) # Assuming train_input is not needed here\n","    val_input = data_loaded['val_input'].to(device)\n","    # train_targets = data_loaded['train_targets'].to(device) # Assuming train_targets is not needed here\n","    val_targets = data_loaded['val_targets'].to(device)\n","    print(f\"‚úÖ Validation data loaded from: {data_save_path}\")\n","    print(f\"   Val: {val_input.shape}\")\n","\n","except FileNotFoundError:\n","    print(f\"‚ùå Error: Data file not found at {data_save_path}. Please ensure the file exists.\")\n","    # You might want to exit or handle this error appropriately\n","    raise"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"orhe6BInALGz","executionInfo":{"status":"ok","timestamp":1760265562547,"user_tz":-330,"elapsed":4373,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"1afca5e8-a625-401c-88fc-46b0f066bb2a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Validation data loaded from: /content/drive/MyDrive/amazon_ml_challenge/combined_CLIP_final/clip_full_with_alignment.pt\n","   Val: torch.Size([15000, 1046])\n"]}]},{"cell_type":"code","source":["\n","\n","\n","# =========================================================================\n","# STEP 1: Generate predictions from all base models (on validation set)\n","# =========================================================================\n","print(\"\\nüìä Generating base model predictions...\")\n","\n","# Data is already on the correct device after loading\n","\n","# Collect individual predictions (in log-space first)\n","base_predictions_log = []\n","max_price = 4000\n","max_log_value = np.log1p(max_price)\n","\n","for i, model in enumerate(models_large):\n","    model.eval()\n","    with torch.no_grad():\n","        pred = model(val_input)\n","        pred_clipped = torch.clamp(pred, min=0, max=max_log_value)\n","        base_predictions_log.append(pred_clipped.cpu().numpy().squeeze())\n","    print(f\"  ‚úì Model {i+1}/10 predictions collected\")\n","\n","# Convert to numpy array: shape (10, num_samples)\n","base_predictions_log = np.array(base_predictions_log)\n","print(f\"\\n‚úÖ Base predictions shape: {base_predictions_log.shape}\")\n","\n","# =========================================================================\n","# STEP 2: Create meta-features\n","# =========================================================================\n","print(\"\\nüîß Engineering meta-features...\")\n","\n","# Basic features: individual model predictions (in original scale)\n","base_preds_orig = np.expm1(base_predictions_log)  # (10, num_samples)\n","\n","# Statistical features across models\n","meta_features = []\n","\n","# 1. Individual predictions\n","for i in range(10):\n","    meta_features.append(base_preds_orig[i])\n","\n","# 2. Ensemble statistics\n","meta_features.append(base_preds_orig.mean(axis=0))      # Mean\n","meta_features.append(base_preds_orig.std(axis=0))       # Std (confidence)\n","meta_features.append(base_preds_orig.min(axis=0))       # Min\n","meta_features.append(base_preds_orig.max(axis=0))       # Max\n","meta_features.append(base_preds_orig.max(axis=0) - base_preds_orig.min(axis=0))  # Range\n","\n","# 3. Agreement metrics\n","# High variance = models disagree = less confident\n","coefficient_of_variation = base_preds_orig.std(axis=0) / (base_preds_orig.mean(axis=0) + 1e-6)\n","meta_features.append(coefficient_of_variation)\n","\n","# 4. Percentiles\n","meta_features.append(np.percentile(base_preds_orig, 25, axis=0))  # Q1\n","meta_features.append(np.percentile(base_preds_orig, 75, axis=0))  # Q3\n","\n","# Stack all features: (num_samples, num_features)\n","X_meta = np.column_stack(meta_features)\n","y_meta = np.expm1(val_targets.cpu().numpy().squeeze())\n","\n","print(f\"‚úÖ Meta-features shape: {X_meta.shape}\")\n","print(f\"   Features: 10 individual preds + {X_meta.shape[1]-10} statistical features\")\n","\n","# =========================================================================\n","# STEP 3: Train multiple meta-learners and select best\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèãÔ∏è Training Meta-Learners\")\n","print(\"=\"*70)\n","\n","meta_models = {}\n","\n","# 1. Ridge Regression (L2 regularization)\n","print(\"\\n1Ô∏è‚É£ Ridge Regression...\")\n","ridge = Ridge(alpha=1.0)\n","ridge.fit(X_meta, y_meta)\n","ridge_pred = ridge.predict(X_meta)\n","ridge_mae = mean_absolute_error(y_meta, ridge_pred)\n","meta_models['ridge'] = ridge\n","print(f\"   MAE: ${ridge_mae:.2f}\")\n","\n","# 2. Lasso Regression (L1 regularization, feature selection)\n","print(\"\\n2Ô∏è‚É£ Lasso Regression...\")\n","lasso = Lasso(alpha=0.1, max_iter=5000)\n","lasso.fit(X_meta, y_meta)\n","lasso_pred = lasso.predict(X_meta)\n","lasso_mae = mean_absolute_error(y_meta, lasso_pred)\n","meta_models['lasso'] = lasso\n","print(f\"   MAE: ${lasso_mae:.2f}\")\n","print(f\"   Features selected: {np.sum(np.abs(lasso.coef_) > 1e-5)} / {len(lasso.coef_)}\")\n","\n","# 3. Random Forest (non-linear meta-learner)\n","print(\"\\n3Ô∏è‚É£ Random Forest...\")\n","rf = RandomForestRegressor(\n","    n_estimators=100,\n","    max_depth=5,\n","    min_samples_split=20,\n","    random_state=42,\n","    n_jobs=-1  # Use all CPU cores\n",")\n","rf.fit(X_meta, y_meta)\n","rf_pred = rf.predict(X_meta)\n","rf_mae = mean_absolute_error(y_meta, rf_pred)\n","meta_models['rf'] = rf\n","print(f\"   MAE: ${rf_mae:.2f}\")\n","\n","# 4. Simple weighted average (baseline)\n","simple_avg = base_preds_orig.mean(axis=0)\n","simple_mae = mean_absolute_error(y_meta, simple_avg)\n","print(f\"\\nüìä Simple Average (baseline): ${simple_mae:.2f}\")\n","\n","# =========================================================================\n","# STEP 4: Select best meta-model\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèÜ META-MODEL COMPARISON\")\n","print(\"=\"*70)\n","\n","results = {\n","    'Simple Average': simple_mae,\n","    'Ridge': ridge_mae,\n","    'Lasso': lasso_mae,\n","    'Random Forest': rf_mae\n","}\n","\n","for name, mae in sorted(results.items(), key=lambda x: x[1]):\n","    improvement = ((simple_mae - mae) / simple_mae) * 100\n","    symbol = \"üéâ\" if mae < simple_mae else \"‚ö†Ô∏è\"\n","    print(f\"{symbol} {name:20} | MAE: ${mae:.2f} | Improvement: {improvement:+.2f}%\")\n","\n","best_name = min(results.items(), key=lambda x: x[1])[0]\n","best_model_key = best_name.lower().replace(' ', '_')\n","if best_model_key == 'simple_average':\n","    best_meta_model = None\n","else:\n","    # Handle case where key might not exactly match after replace (e.g., 'RandomForest' vs 'random_forest')\n","    # Find the actual key in meta_models\n","    actual_best_key = None\n","    for k in meta_models.keys():\n","        if k.replace('_', '') == best_model_key.replace('_', ''):\n","            actual_best_key = k\n","            break\n","\n","    if actual_best_key:\n","        best_meta_model = meta_models[actual_best_key]\n","    else:\n","        best_meta_model = None # Should not happen if logic is correct\n","\n","\n","print(f\"\\n‚ú® Best meta-model: {best_name}\")\n","\n","# =========================================================================\n","# STEP 5: Analyze feature importance (if using tree-based model)\n","# =========================================================================\n","if best_name == 'Random Forest':\n","    print(\"\\n\" + \"=\"*70)\n","    print(\"üîç FEATURE IMPORTANCE\")\n","    print(\"=\"*70)\n","\n","    feature_names = (\n","        [f\"Model_{i+1}\" for i in range(10)] +\n","        ['Mean', 'Std', 'Median', 'Min', 'Max', 'Range', 'CoefVar', 'Q1', 'Q3']\n","    )\n","\n","    importances = rf.feature_importances_\n","    indices = np.argsort(importances)[::-1][:10]  # Top 10\n","\n","    print(\"\\nTop 10 Most Important Features:\")\n","    for i, idx in enumerate(indices):\n","        print(f\"  {i+1}. {feature_names[idx]:15} | Importance: {importances[idx]:.4f}\")\n","\n","# =========================================================================\n","# STEP 6: Error analysis by price range\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìä ERROR ANALYSIS BY PRICE RANGE\")\n","print(\"=\"*70)\n","\n","if best_meta_model is not None:\n","    best_pred = best_meta_model.predict(X_meta)\n","else:\n","    best_pred = simple_avg\n","\n","price_ranges = [\n","    (0, 10, \"$0-$10\"),\n","    (10, 20, \"$10-$20\"),\n","    (20, 50, \"$20-$50\"),\n","    (50, 100, \"$50-$100\"),\n","    (100, float('inf'), \"$100+\")\n","]\n","\n","print(f\"\\n{'Range':12} | {'Count':>5} | {'Simple Avg':>12} | {best_name:>12}\")\n","print(\"-\" * 70)\n","\n","for low, high, label in price_ranges:\n","    mask = (y_meta >= low) & (y_meta < high)\n","    if mask.sum() > 0:\n","        simple_range_mae = np.abs(y_meta[mask] - simple_avg[mask]).mean()\n","        best_range_mae = np.abs(y_meta[mask] - best_pred[mask]).mean()\n","        count = mask.sum()\n","\n","        improvement = simple_range_mae - best_range_mae\n","        symbol = \"‚Üì\" if improvement > 0 else \"‚Üë\"\n","        print(f\"{label:12} | {count:5} | ${simple_range_mae:11.2f} | ${best_range_mae:11.2f} {symbol}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rVlldx4wtuYM","executionInfo":{"status":"ok","timestamp":1760265675266,"user_tz":-330,"elapsed":29189,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"a74853a6-b8e3-48ed-dc1f-a28c7bc15275"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","üìä Generating base model predictions...\n","  ‚úì Model 1/10 predictions collected\n","  ‚úì Model 2/10 predictions collected\n","  ‚úì Model 3/10 predictions collected\n","  ‚úì Model 4/10 predictions collected\n","  ‚úì Model 5/10 predictions collected\n","  ‚úì Model 6/10 predictions collected\n","  ‚úì Model 7/10 predictions collected\n","  ‚úì Model 8/10 predictions collected\n","  ‚úì Model 9/10 predictions collected\n","  ‚úì Model 10/10 predictions collected\n","\n","‚úÖ Base predictions shape: (10, 15000)\n","\n","üîß Engineering meta-features...\n","‚úÖ Meta-features shape: (15000, 18)\n","   Features: 10 individual preds + 8 statistical features\n","\n","======================================================================\n","üèãÔ∏è Training Meta-Learners\n","======================================================================\n","\n","1Ô∏è‚É£ Ridge Regression...\n","   MAE: $13.11\n","\n","2Ô∏è‚É£ Lasso Regression...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_coordinate_descent.py:695: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.308e+06, tolerance: 2.264e+03\n","  model = cd_fast.enet_coordinate_descent(\n"]},{"output_type":"stream","name":"stdout","text":["   MAE: $13.08\n","   Features selected: 18 / 18\n","\n","3Ô∏è‚É£ Random Forest...\n","   MAE: $12.59\n","\n","üìä Simple Average (baseline): $12.28\n","\n","======================================================================\n","üèÜ META-MODEL COMPARISON\n","======================================================================\n","‚ö†Ô∏è Simple Average       | MAE: $12.28 | Improvement: +0.00%\n","‚ö†Ô∏è Random Forest        | MAE: $12.59 | Improvement: -2.58%\n","‚ö†Ô∏è Lasso                | MAE: $13.08 | Improvement: -6.52%\n","‚ö†Ô∏è Ridge                | MAE: $13.11 | Improvement: -6.78%\n","\n","‚ú® Best meta-model: Simple Average\n","\n","======================================================================\n","üìä ERROR ANALYSIS BY PRICE RANGE\n","======================================================================\n","\n","Range        | Count |   Simple Avg | Simple Average\n","----------------------------------------------------------------------\n","$0-$10       |  5746 | $       5.42 | $       5.42 ‚Üë\n","$10-$20      |  3837 | $       4.89 | $       4.89 ‚Üë\n","$20-$50      |  3790 | $      12.76 | $      12.76 ‚Üë\n","$50-$100     |  1235 | $      34.76 | $      34.76 ‚Üë\n","$100+        |   392 | $     109.57 | $     109.57 ‚Üë\n"]}]},{"cell_type":"markdown","source":["## Unsupervised Category Discovery via Clustering"],"metadata":{"id":"rXzVq9l7Enyt"}},{"cell_type":"code","source":["# =========================================================================\n","# Unsupervised Category Discovery via Clustering\n","# Use CLIP text embeddings to discover product categories\n","# =========================================================================\n","import torch\n","import numpy as np\n","import pandas as pd\n","from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","from collections import Counter\n","import seaborn as sns\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"üîç UNSUPERVISED CATEGORY DISCOVERY\")\n","print(\"=\"*70)\n","\n","# =========================================================================\n","# Load your CLIP text embeddings\n","# =========================================================================\n","# Assuming you have:\n","# clip_text_embeddings: (num_samples, 512) - CLIP text features\n","# train_texts: list of product descriptions\n","# targets: prices in log scale\n","base_dir = '/content/drive/MyDrive/amazon_ml_challenge'\n","clip_full_embeddings = torch.load(\"/content/drive/MyDrive/amazon_ml_challenge/combined_CLIP_final/clip_full_with_alignment.pt\")\n","processed_csv_path = os.path.join(base_dir, \"processed_fe+textpre.csv\")\n","data = pd.read_csv(processed_csv_path)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rSjgDWZEDH8S","executionInfo":{"status":"ok","timestamp":1760268270989,"user_tz":-330,"elapsed":3677,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}},"outputId":"129f52a7-b91e-46e8-9569-3e8249067ab5"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","======================================================================\n","üîç UNSUPERVISED CATEGORY DISCOVERY\n","======================================================================\n"]}]},{"cell_type":"code","source":["train_texts=data.catalog_content\n","targets=data.price"],"metadata":{"id":"Dsr-FKQJKQ_3","executionInfo":{"status":"ok","timestamp":1760268271017,"user_tz":-330,"elapsed":10,"user":{"displayName":"Nitish Rishi","userId":"10781579516426583515"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nr40DFQ8KkgJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","print(\"\\nüìä Data Info:\")\n","print(f\"Text embeddings shape: {clip_text_embeddings.shape}\")\n","print(f\"Number of samples: {len(train_texts)}\")\n","\n","# Convert to numpy for sklearn\n","X_text = clip_text_embeddings.cpu().numpy() if torch.is_tensor(clip_text_embeddings) else clip_text_embeddings\n","prices_orig = np.expm1(targets.cpu().numpy() if torch.is_tensor(targets) else targets)\n","\n","# =========================================================================\n","# STEP 1: Determine optimal number of clusters\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìà Finding Optimal Number of Clusters\")\n","print(\"=\"*70)\n","\n","# Try different K values\n","K_range = range(5, 31, 5)  # Test 5, 10, 15, 20, 25, 30 clusters\n","inertias = []\n","silhouette_scores = []\n","\n","from sklearn.metrics import silhouette_score\n","\n","print(\"\\nTesting different K values (this may take a few minutes)...\")\n","for k in K_range:\n","    print(f\"  Testing K={k}...\", end=\" \")\n","    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10, max_iter=100)\n","    labels = kmeans.fit_predict(X_text)\n","    inertias.append(kmeans.inertia_)\n","\n","    # Silhouette score (sample for speed if dataset is large)\n","    if len(X_text) > 10000:\n","        sample_idx = np.random.choice(len(X_text), 10000, replace=False)\n","        score = silhouette_score(X_text[sample_idx], labels[sample_idx])\n","    else:\n","        score = silhouette_score(X_text, labels)\n","    silhouette_scores.append(score)\n","    print(f\"Silhouette: {score:.3f}\")\n","\n","# Plot elbow curve\n","print(\"\\nüìä Elbow Method Analysis:\")\n","for k, inertia, sil in zip(K_range, inertias, silhouette_scores):\n","    print(f\"  K={k:2d} | Inertia: {inertia:12.0f} | Silhouette: {sil:.3f}\")\n","\n","# Suggest optimal K (highest silhouette score)\n","optimal_k = K_range[np.argmax(silhouette_scores)]\n","print(f\"\\n‚ú® Suggested K: {optimal_k} (highest silhouette score)\")\n","\n","# =========================================================================\n","# STEP 2: Perform clustering with optimal K\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(f\"üéØ Clustering with K={optimal_k}\")\n","print(\"=\"*70)\n","\n","# KMeans clustering\n","kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=20, max_iter=300)\n","cluster_labels = kmeans.fit_predict(X_text)\n","\n","print(f\"‚úÖ Clustering complete!\")\n","print(f\"\\nCluster distribution:\")\n","cluster_counts = Counter(cluster_labels)\n","for cluster_id in sorted(cluster_counts.keys()):\n","    count = cluster_counts[cluster_id]\n","    pct = (count / len(cluster_labels)) * 100\n","    print(f\"  Cluster {cluster_id:2d}: {count:5d} samples ({pct:5.1f}%)\")\n","\n","# =========================================================================\n","# STEP 3: Analyze clusters - discover what they represent\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üîç CLUSTER ANALYSIS\")\n","print(\"=\"*70)\n","\n","def analyze_cluster(cluster_id, texts, prices, labels, top_n=10):\n","    \"\"\"Analyze what a cluster represents\"\"\"\n","    mask = labels == cluster_id\n","    cluster_texts = [texts[i] for i in range(len(texts)) if mask[i]]\n","    cluster_prices = prices[mask]\n","\n","    print(f\"\\n{'='*70}\")\n","    print(f\"CLUSTER {cluster_id} - {mask.sum()} samples\")\n","    print(f\"{'='*70}\")\n","\n","    # Price statistics\n","    print(f\"\\nüí∞ Price Statistics:\")\n","    print(f\"  Mean: ${cluster_prices.mean():.2f}\")\n","    print(f\"  Median: ${np.median(cluster_prices):.2f}\")\n","    print(f\"  Min: ${cluster_prices.min():.2f}\")\n","    print(f\"  Max: ${cluster_prices.max():.2f}\")\n","    print(f\"  Std: ${cluster_prices.std():.2f}\")\n","\n","    # Sample texts (to manually identify category)\n","    print(f\"\\nüìù Sample Products (first {top_n}):\")\n","    for i, idx in enumerate(np.where(mask)[0][:top_n]):\n","        text_preview = texts[idx][:100] + \"...\" if len(texts[idx]) > 100 else texts[idx]\n","        print(f\"  {i+1}. [{prices[idx]:6.2f}] {text_preview}\")\n","\n","    # Common words (simple word frequency)\n","    from collections import Counter\n","    import re\n","    all_words = []\n","    for text in cluster_texts[:1000]:  # Sample for speed\n","        words = re.findall(r'\\b[a-z]{3,}\\b', str(text).lower())\n","        all_words.extend(words)\n","\n","    # Filter stopwords\n","    stopwords = {'the', 'and', 'for', 'with', 'this', 'that', 'from', 'are', 'has'}\n","    word_counts = Counter([w for w in all_words if w not in stopwords])\n","\n","    print(f\"\\nüî§ Top Keywords:\")\n","    for word, count in word_counts.most_common(15):\n","        print(f\"  {word:15} ({count})\")\n","\n","# Analyze first few clusters\n","print(\"\\nüî¨ Analyzing clusters to discover categories...\")\n","for cluster_id in range(min(5, optimal_k)):  # Analyze first 5 clusters\n","    analyze_cluster(cluster_id, train_texts, prices_orig, cluster_labels)\n","\n","# =========================================================================\n","# STEP 4: Visualize clusters (2D projection)\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üé® Visualizing Clusters\")\n","print(\"=\"*70)\n","\n","# Use PCA for quick 2D visualization\n","print(\"\\nReducing to 2D with PCA...\")\n","pca = PCA(n_components=2, random_state=42)\n","X_2d = pca.fit_transform(X_text)\n","\n","# Plot\n","plt.figure(figsize=(14, 10))\n","scatter = plt.scatter(X_2d[:, 0], X_2d[:, 1],\n","                     c=cluster_labels,\n","                     cmap='tab20',\n","                     alpha=0.6,\n","                     s=10)\n","plt.colorbar(scatter, label='Cluster ID')\n","plt.title(f'Product Clusters (K={optimal_k}) - PCA Projection')\n","plt.xlabel('PC1')\n","plt.ylabel('PC2')\n","plt.tight_layout()\n","plt.savefig('/content/clusters_pca.png', dpi=150, bbox_inches='tight')\n","print(\"‚úÖ Visualization saved: /content/clusters_pca.png\")\n","\n","# =========================================================================\n","# STEP 5: Analyze price patterns by cluster\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üí∞ PRICE ANALYSIS BY CLUSTER\")\n","print(\"=\"*70)\n","\n","cluster_price_stats = []\n","for cluster_id in range(optimal_k):\n","    mask = cluster_labels == cluster_id\n","    cluster_prices = prices_orig[mask]\n","\n","    stats = {\n","        'cluster': cluster_id,\n","        'count': mask.sum(),\n","        'mean': cluster_prices.mean(),\n","        'median': np.median(cluster_prices),\n","        'std': cluster_prices.std(),\n","        'min': cluster_prices.min(),\n","        'max': cluster_prices.max()\n","    }\n","    cluster_price_stats.append(stats)\n","\n","# Sort by mean price\n","cluster_price_stats.sort(key=lambda x: x['mean'])\n","\n","print(\"\\nClusters sorted by average price:\")\n","print(f\"{'Cluster':>8} | {'Count':>6} | {'Mean':>8} | {'Median':>8} | {'Std':>8}\")\n","print(\"-\" * 70)\n","for stats in cluster_price_stats:\n","    print(f\"{stats['cluster']:8d} | {stats['count']:6d} | \"\n","          f\"${stats['mean']:7.2f} | ${stats['median']:7.2f} | ${stats['std']:7.2f}\")\n","\n","# =========================================================================\n","# STEP 6: Create category features for modeling\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üèóÔ∏è CREATING CATEGORY FEATURES\")\n","print(\"=\"*70)\n","\n","# One-hot encode clusters\n","from sklearn.preprocessing import OneHotEncoder\n","\n","cluster_onehot = np.zeros((len(cluster_labels), optimal_k))\n","for i, label in enumerate(cluster_labels):\n","    cluster_onehot[i, label] = 1\n","\n","print(f\"‚úÖ One-hot encoded clusters: {cluster_onehot.shape}\")\n","\n","# Alternative: Use cluster center distances as features\n","cluster_distances = kmeans.transform(X_text)  # Distance to each cluster center\n","print(f\"‚úÖ Cluster distances: {cluster_distances.shape}\")\n"],"metadata":{"id":"gVXHfwFGG3OQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# =========================================================================\n","# STEP 7: Save results\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üíæ SAVING RESULTS\")\n","print(\"=\"*70)\n","\n","# Save cluster assignments\n","results = {\n","    'cluster_labels': cluster_labels,\n","    'cluster_onehot': cluster_onehot,\n","    'cluster_distances': cluster_distances,\n","    'kmeans_model': kmeans,\n","    'optimal_k': optimal_k,\n","    'cluster_price_stats': cluster_price_stats\n","}\n","\n","save_path = '/content/drive/MyDrive/amazon_ml_challenge/cluster_results.pt'\n","torch.save(results, save_path)\n","print(f\"‚úÖ Results saved to: {save_path}\")\n","\n","# =========================================================================\n","# STEP 8: How to use these features\n","# =========================================================================\n","print(\"\\n\" + \"=\"*70)\n","print(\"üìñ HOW TO USE CLUSTER FEATURES\")\n","print(\"=\"*70)\n","\n","usage_guide = \"\"\"\n","Three ways to use discovered categories:\n","\n","1Ô∏è‚É£ ADD AS FEATURES (Simple):\n","   # Concatenate one-hot clusters to your existing features\n","   enhanced_input = torch.cat([\n","       img_tensor,              # 512\n","       clip_text_embeddings,    # 512\n","       alignment_features,      # 3\n","       structured_features,     # 19\n","       torch.FloatTensor(cluster_onehot)  # K clusters\n","   ], dim=1)\n","\n","   # Now train with 1046 + K features\n","\n","2Ô∏è‚É£ TRAIN SEPARATE MODELS PER CLUSTER (Advanced):\n","   for cluster_id in range(K):\n","       mask = cluster_labels == cluster_id\n","       X_cluster = train_input[mask]\n","       y_cluster = train_targets[mask]\n","\n","       # Train specialist model for this category\n","       model_cluster = RegressionMLP(input_dim).to(device)\n","       train(model_cluster, X_cluster, y_cluster)\n","\n","   # At inference: predict cluster, then use that model\n","\n","3Ô∏è‚É£ CLUSTER-WEIGHTED LOSS (Hybrid):\n","   # Weight loss by cluster difficulty\n","   cluster_stds = [stats['std'] for stats in cluster_price_stats]\n","   weights = torch.FloatTensor([cluster_stds[label] for label in cluster_labels])\n","\n","   loss = (weights * criterion(pred, target)).mean()\n","\"\"\"\n","\n","print(usage_guide)\n","\n","print(\"\\n\" + \"=\"*70)\n","print(\"‚úÖ CLUSTERING COMPLETE!\")\n","print(\"=\"*70)\n","print(f\"\\nüéØ Discovered {optimal_k} product categories\")\n","print(f\"üìä Ready to enhance your model with category features!\")\n","print(f\"üí° Next: Try approach #1 (add as features) or #2 (separate models)\")"],"metadata":{"id":"qIYP83q6DNH4"},"execution_count":null,"outputs":[]}]}